{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks (Continued)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'Week11M'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-afacb5e7e619>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mWeek11M\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'Week11M'"
     ]
    }
   ],
   "source": [
    "from Week11M import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually we don’t build neural networks by hand. This is in part because we use them\n",
    "to solve much bigger problems—an image recognition problem might involve hundreds\n",
    "or thousands of neurons. And it’s in part because we usually won’t be able to\n",
    "“reason out” what the neurons should be.\n",
    "\n",
    "Instead (as usual) we use data to *train* neural networks. The typical approach is an\n",
    "algorithm called *backpropagation*, which uses gradient descent or one of its variants."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine we have a training set that consists of input vectors and corresponding target\n",
    "output vectors.\n",
    "\n",
    "For example, in our previous `xor_network` example, the input vector\n",
    "[1, 0] corresponded to the target output [1]. Imagine that our network has some set\n",
    "of weights. We then adjust the weights using the following algorithm:\n",
    "\n",
    "1. Run `feed_forward` on an input vector to produce the outputs of all the neurons in the network.\n",
    "2. We know the target output, so we can compute a `loss` that’s the sum of the squared errors.\n",
    "3. Compute the gradient of this loss as a function of the output neuron’s weights.\n",
    "4. “Propagate” the gradients and errors backward to compute the gradients with respect to the hidden neurons’ weights.\n",
    "5. Take a gradient descent step.\n",
    "\n",
    "Typically we run this algorithm many times for our entire training set until the network\n",
    "converges.\n",
    "\n",
    "The actual math involved can be quite tedious and requires some solid background in multivariate calculus, especially chain rule, as well as familiarity with matrix and vector calculus.\n",
    "\n",
    "For our purposes, the results from computing the gradients layer by layer are as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sqerror_gradients(network: List[List[Vector]],\n",
    "                      input_vector: Vector,\n",
    "                      target_vector: Vector) -> List[List[Vector]]:\n",
    "    \"\"\"\n",
    "    Given a neural network, an input vector, and a target vector,\n",
    "    make a prediction and compute the gradient of the squared error\n",
    "    loss with respect to the neuron weights.\n",
    "    \"\"\"\n",
    "    # forward pass\n",
    "    hidden_outputs, outputs = feed_forward(network, input_vector)\n",
    "\n",
    "    # gradients with respect to output neuron pre-activation outputs\n",
    "    output_deltas = [output * (1 - output) * (output - target)\n",
    "                     for output, target in zip(outputs, target_vector)]\n",
    "\n",
    "    # gradients with respect to output neuron weights\n",
    "    output_grads = [[output_deltas[i] * hidden_output\n",
    "                     for hidden_output in hidden_outputs + [1]]\n",
    "                    for i, output_neuron in enumerate(network[-1])]\n",
    "\n",
    "    # gradients with respect to hidden neuron pre-activation outputs\n",
    "    hidden_deltas = [hidden_output * (1 - hidden_output) *\n",
    "                         np.dot(output_deltas, [n[i] for n in network[-1]])\n",
    "                     for i, hidden_output in enumerate(hidden_outputs)]\n",
    "\n",
    "    # gradients with respect to hidden neuron weights\n",
    "    hidden_grads = [[hidden_deltas[i] * input for input in input_vector + [1]]\n",
    "                    for i, hidden_neuron in enumerate(network[0])]\n",
    "\n",
    "    return [hidden_grads, output_grads]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the gradients, we can train neural networks. Let’s\n",
    "try to learn the XOR network we previously designed by hand.\n",
    "\n",
    "We’ll start by generating the training data and initializing our neural network with\n",
    "random weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(0)\n",
    "\n",
    "# training data\n",
    "xs = [[0, 0], [0, 1], [1, 0], [1, 1]]\n",
    "ys = [[0], [1], [1], [0]]\n",
    "\n",
    "# start with random weights\n",
    "network = [ # hidden layer: 2 inputs -> 2 outputs\n",
    "            [[random.random() for _ in range(2 + 1)],   # 1st hidden neuron\n",
    "             [random.random() for _ in range(2 + 1)]],  # 2nd hidden neuron\n",
    "            # output layer: 2 inputs -> 1 output\n",
    "            [[random.random() for _ in range(2 + 1)]]   # 1st output neuron\n",
    "          ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, we can train it using gradient descent. One difference from our previous\n",
    "examples is that here we have several parameter vectors, each with its own gradient,\n",
    "which means we’ll have to call `gradient_step` for each of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Week10TR import gradient_step\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1.0\n",
    "\n",
    "for epoch in tqdm.trange(20000, desc=\"neural net for xor\"):\n",
    "    for x, y in zip(xs, ys):\n",
    "        gradients = sqerror_gradients(network, x, y)\n",
    "\n",
    "        # Take a gradient step for each neuron in each layer\n",
    "        network = [[gradient_step(neuron, grad, -learning_rate)\n",
    "                    for neuron, grad in zip(layer, layer_grad)]\n",
    "                   for layer, layer_grad in zip(network, gradients)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that it learned XOR\n",
    "\n",
    "print(f\"1 XOR 1 = {feed_forward(network, [1, 1])[-1][0]}\")\n",
    "print(f\"0 XOR 1 = {feed_forward(network, [0, 1])[-1][0]}\")\n",
    "print(f\"1 XOR 0 = {feed_forward(network, [1, 0])[-1][0]}\")\n",
    "print(f\"0 XOR 0 = {feed_forward(network, [0, 0])[-1][0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Fizz Buzz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The VP of Engineering wants to interview technical candidates by making them solve\n",
    "“Fizz Buzz,” the following well-trod programming challenge:\n",
    "\n",
    "```\n",
    "Print the numbers 1 to 100, except that if the number is divisible\n",
    "by 3, print \"fizz\"; if the number is divisible by 5, print \"buzz\";\n",
    "and if the number is divisible by 15, print \"fizzbuzz\".\n",
    "```\n",
    "\n",
    "He thinks the ability to solve this demonstrates extreme programming skill. You think\n",
    "that this problem is so simple that a neural network could solve it.\n",
    "\n",
    "Neural networks take vectors as inputs and produce vectors as outputs. As stated, the\n",
    "programming problem is to turn an integer into a string. So the first challenge is to\n",
    "come up with a way to recast it as a vector problem.\n",
    "\n",
    "For the outputs it’s not tough: there are basically four classes of outputs, so we can\n",
    "encode the output as a vector of four 0s and 1s:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fizz_buzz_encode(x: int) -> Vector:\n",
    "    if x % 15 == 0:\n",
    "        return [0, 0, 0, 1]\n",
    "    elif x % 5 == 0:\n",
    "        return [0, 0, 1, 0]\n",
    "    elif x % 3 == 0:\n",
    "        return [0, 1, 0, 0]\n",
    "    else:\n",
    "        return [1, 0, 0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fizz_buzz_encode(2))  # [1, 0, 0, 0]\n",
    "print(fizz_buzz_encode(6))  # [0, 1, 0, 0]\n",
    "print(fizz_buzz_encode(10)) # [0, 0, 1, 0]\n",
    "print(fizz_buzz_encode(30)) # [0, 0, 0, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’ll use this to generate our target vectors. The input vectors are less obvious.\n",
    "\n",
    "You\n",
    "don’t want to just use a one-dimensional vector containing the input number, for a\n",
    "couple of reasons.\n",
    "- A single input captures an “intensity,” but the fact that 2 is twice as much as 1, and that 4 is twice as much again, doesn’t feel relevant to this problem.\n",
    "- Additionally, with just one input the hidden layer wouldn’t be able to compute very interesting features, which means it probably wouldn’t be able to solve the problem.\n",
    "\n",
    "It turns out that one thing that works reasonably well is to convert each number to its\n",
    "binary representation of 1s and 0s (the exact reasoning for this is not obvious, and we don't need to dig into it either)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_encode(x: int) -> Vector:\n",
    "    binary: List[float] = []\n",
    "\n",
    "    for i in range(10):\n",
    "        binary.append(x % 2)\n",
    "        x = x // 2\n",
    "\n",
    "    return binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#                             1  2  4  8 16 32 64 128 256 512\n",
    "assert binary_encode(0)   == [0, 0, 0, 0, 0, 0, 0, 0,  0,  0]\n",
    "assert binary_encode(1)   == [1, 0, 0, 0, 0, 0, 0, 0,  0,  0]\n",
    "assert binary_encode(10)  == [0, 1, 0, 1, 0, 0, 0, 0,  0,  0]\n",
    "assert binary_encode(101) == [1, 0, 1, 0, 0, 1, 1, 0,  0,  0]\n",
    "assert binary_encode(999) == [1, 1, 1, 0, 0, 1, 1, 1,  1,  1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the goal is to construct the outputs for the numbers 1 to 100, it would be cheating\n",
    "to train on those numbers. Therefore, we’ll train on the numbers 101 to 1,023 (which\n",
    "is the largest number we can represent with 10 binary digits):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = [binary_encode(n) for n in range(101, 1024)]\n",
    "ys = [fizz_buzz_encode(n) for n in range(101, 1024)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let’s create a neural network with random initial weights. It will have 10 input\n",
    "neurons (since we’re representing our inputs as 10-dimensional vectors) and 4 output\n",
    "neurons (since we’re representing our targets as 4-dimensional vectors). We’ll give it\n",
    "25 hidden units, but we’ll use a variable for that so it’s easy to change:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_HIDDEN = 25\n",
    "\n",
    "network = [\n",
    "    # hidden layer: 10 inputs -> NUM_HIDDEN outputs\n",
    "    [[random.random() for _ in range(10 + 1)] for _ in range(NUM_HIDDEN)],\n",
    "\n",
    "    # output_layer: NUM_HIDDEN inputs -> 4 outputs\n",
    "    [[random.random() for _ in range(NUM_HIDDEN + 1)] for _ in range(4)]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That’s it. Now we’re ready to train. Because this is a more involved problem (and\n",
    "there are a lot more things to mess up), we’d like to closely monitor the training process.\n",
    "In particular, for each epoch we’ll track the sum of squared errors and print\n",
    "them out. We want to make sure they decrease:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1.0\n",
    "\n",
    "with tqdm.trange(500) as t:\n",
    "    for epoch in t:\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for x, y in zip(xs, ys):\n",
    "            predicted = feed_forward(network, x)[-1]\n",
    "            error = np.subtract(y, predicted)\n",
    "            epoch_loss += np.dot(error, error)\n",
    "            gradients = sqerror_gradients(network, x, y)\n",
    "\n",
    "            # Take a gradient step for each neuron in each layer\n",
    "            network = [[gradient_step(neuron, grad, -learning_rate)\n",
    "                        for neuron, grad in zip(layer, layer_grad)]\n",
    "                    for layer, layer_grad in zip(network, gradients)]\n",
    "\n",
    "        t.set_description(f\"fizz buzz (loss: {epoch_loss:.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will take a while to train, but eventually the loss should start to bottom out.\n",
    "\n",
    "At last we’re ready to solve our original problem. We have one remaining issue. Our\n",
    "network will produce a four-dimensional vector of numbers, but we want a single\n",
    "prediction. We’ll do that by taking the `argmax`, which is the index of the largest value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def argmax(xs: list) -> int:\n",
    "    \"\"\"Returns the index of the largest value\"\"\"\n",
    "    return max(range(len(xs)), key=lambda i: xs[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(argmax([0, -1]))               # items[0] is largest\n",
    "print(argmax([-1, 0]))               # items[1] is largest\n",
    "print(argmax([-1, 10, 5, 20, -3]))   # items[3] is largest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can finally solve “FizzBuzz”:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_correct = 0\n",
    "\n",
    "for n in range(1, 101):\n",
    "    x = binary_encode(n)\n",
    "    predicted = argmax(feed_forward(network, x)[-1])\n",
    "    actual = argmax(fizz_buzz_encode(n))\n",
    "    labels = [str(n), \"fizz\", \"buzz\", \"fizzbuzz\"]\n",
    "    print(n, labels[predicted], labels[actual])\n",
    "\n",
    "    if predicted == actual:\n",
    "        num_correct += 1\n",
    "\n",
    "print(num_correct, \"/\", 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the neural network is able to perform quite well on this particular problem. The performance could be potentially improve if we train more for more epochs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
