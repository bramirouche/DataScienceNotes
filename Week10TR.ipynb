{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Frequently when doing data science, we’ll be trying to the find the best model for a\n",
    "certain situation. And usually “best” will mean something like “minimizes the error\n",
    "of its predictions” In other words, it will\n",
    "represent the solution to some sort of optimization problem.\n",
    "\n",
    "This means we’ll need to solve a number of optimization problems. Our approach will be a technique called gradient\n",
    "descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Idea Behind Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we have some function `f` that takes as input a vector of real numbers and\n",
    "outputs a single real number. One simple such function is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import List\n",
    "Vector = List[float]\n",
    "\n",
    "def sum_of_squares(v: Vector) -> float:\n",
    "    \"\"\"Computes the sum of squared elements in v\"\"\"\n",
    "    return np.dot(v, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’ll frequently need to maximize or minimize such functions. That is, we need to\n",
    "find the input `v` that produces the largest (or smallest) possible value.\n",
    "\n",
    "For functions like ours, the gradient (which is the vector of partial derivatives) gives the input direction in which the function most quickly increases. (If you don’t know your calculus, take my word for it or look it up on\n",
    "the internet.)\n",
    "\n",
    "Accordingly, one approach to maximizing a function is to pick a random starting\n",
    "point, compute the gradient, take a small step in the direction of the gradient (i.e., the\n",
    "direction that causes the function to increase the most), and repeat with the new\n",
    "starting point. Similarly, you can try to minimize a function by taking small steps in\n",
    "the opposite direction.\n",
    "\n",
    "![Finding a minimum using gradient descent](finding_min.jpg)\n",
    "\n",
    "Note: If a function has a unique global minimum, this procedure is likely\n",
    "to find it. If a function has multiple (local) minima, this procedure\n",
    "might “find” the wrong one of them, in which case you might rerun\n",
    "the procedure from different starting points. If a function has no\n",
    "minimum, then it’s possible the procedure might go on forever."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating the Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If `f` is a function of one variable, its derivative at a point `x` measures how `f(x)`\n",
    "changes when we make a very small change to `x`. The derivative is defined as the limit\n",
    "of the difference quotients as `h`, the small change to `x`, approaches 0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "\n",
    "def difference_quotient(f: Callable[[float], float],\n",
    "                        x: float,\n",
    "                        h: float) -> float:\n",
    "    return (f(x + h) - f(x)) / h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The derivative is the slope of the tangent line at `(x, f(x))` , while the difference quotient\n",
    "is the slope of the not-quite-tangent line that runs through `(x + h, f(x + h))` . As `h`\n",
    "gets smaller and smaller, the not-quite-tangent line gets closer and closer to the tangent\n",
    "line.\n",
    "\n",
    "![Approximating a derivative with a difference quotient](approx_derivative.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For many functions it’s easy to exactly calculate derivatives. For example, the `square`\n",
    "function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def square(x: float) -> float:\n",
    "    return x * x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "has the derivative:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivative(x: float) -> float:\n",
    "    return 2 * x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which is easy for us to check by explicitly computing the difference quotient and taking\n",
    "the limit.\n",
    "\n",
    "What if you couldn’t (or didn’t want to) find the gradient? Although we can’t take limits\n",
    "in Python, we can estimate derivatives by evaluating the difference quotient for a\n",
    "very small `h`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = range(-10, 11)\n",
    "actuals = [derivative(x) for x in xs]\n",
    "estimates = [difference_quotient(square, x, h=0.001) for x in xs]\n",
    "\n",
    "# plot to show they're basically the same\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.title(\"Actual Derivatives vs. Estimates\")\n",
    "plt.plot(xs, actuals, 'rx', label='Actual')     # red x\n",
    "plt.plot(xs, estimates, 'b+', label='Estimate') # blue +\n",
    "plt.legend(loc=9)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When `f` is a function of many variables, it has multiple partial derivatives, each indicating\n",
    "how `f` changes when we make small changes in just one of the input variables.\n",
    "\n",
    "We calculate (or estimate) its `i`th partial derivative by treating it as a function of just its `i`th variable,\n",
    "holding the other variables fixed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partial_difference_quotient(f: Callable[[Vector], float],\n",
    "                                v: Vector,\n",
    "                                i: int,\n",
    "                                h: float) -> float:\n",
    "    \"\"\"Returns the i-th partial difference quotient of f at v\"\"\"\n",
    "    w = [v_j + (h if j == i else 0)    # add h to just the ith element of v\n",
    "         for j, v_j in enumerate(v)]\n",
    "\n",
    "    return (f(w) - f(v)) / h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "after which we can estimate the gradient the same way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_gradient(f: Callable[[Vector], float],\n",
    "                      v: Vector,\n",
    "                      h: float = 0.0001):\n",
    "    return [partial_difference_quotient(f, v, i, h)\n",
    "            for i in range(len(v))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A major drawback to this “estimate using difference quotients”\n",
    "approach is that it’s computationally expensive. If `v` has length `n`,\n",
    "`estimate_gradient` has to evaluate `f` on `2n` different inputs. If\n",
    "you’re repeatedly estimating gradients, you’re doing a whole lot of\n",
    "extra work. Often times, we’ll use math to calculate our\n",
    "gradient functions explicitly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It’s easy to see that the `sum_of_squares` function is smallest when its input `v` is a vector\n",
    "of zeros. But imagine we didn’t know that.\n",
    "\n",
    "Let’s use gradients to find the minimum\n",
    "among all three-dimensional vectors. We’ll just pick a random starting point\n",
    "and then take tiny steps in the opposite direction of the gradient until we reach a\n",
    "point where the gradient is very small:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_step(v: Vector, gradient: Vector, step_size: float) -> Vector:\n",
    "    \"\"\"Moves `step_size` in the `gradient` direction from `v`\"\"\"\n",
    "    assert len(v) == len(gradient)\n",
    "    step = np.multiply(step_size, gradient)\n",
    "    return np.add(v, step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_of_squares_gradient(v: Vector) -> Vector:\n",
    "    return [2 * v_i for v_i in v]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# pick a random starting point\n",
    "v = [random.uniform(-10, 10) for i in range(3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1000):\n",
    "    grad = sum_of_squares_gradient(v)    # compute the gradient at v\n",
    "    v = gradient_step(v, grad, -0.01)    # take a negative gradient step\n",
    "    print(epoch, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance\n",
    "\n",
    "assert distance.euclidean(v, [0, 0, 0]) < 0.001    # v should be close to 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you run this, you’ll find that it always ends up with a `v` that’s very close to `[0,0,0]`.\n",
    "The more epochs you run it for, the closer it will get."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing the Right Step Size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the rationale for moving against the gradient is clear, how far to move is\n",
    "not. Indeed, choosing the right step size is more of an art than a science. Popular\n",
    "options include:\n",
    "- Using a fixed step size\n",
    "- Gradually shrinking the step size over time\n",
    "- At each step, choosing the step size that minimizes the value of the objective function\n",
    "\n",
    "The last approach sounds great but is, in practice, a costly computation.\n",
    "\n",
    "To keep\n",
    "things simple, we’ll mostly just use a fixed step size. The step size that “works”\n",
    "depends on the problem—too small, and your gradient descent will take forever; too\n",
    "big, and you’ll take giant steps that might make the function you care about get larger\n",
    "or even be undefined. So we’ll need to experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Gradient Descent to Fit Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’ll be using gradient descent to fit parameterized models to data.\n",
    "\n",
    "In\n",
    "the usual case, we’ll have some dataset and some (hypothesized) model for the data\n",
    "that depends (in a differentiable way) on one or more parameters.\n",
    "\n",
    "We’ll also have a\n",
    "loss function that measures how well the model fits our data. (e.g., sse, smaller is better.)\n",
    "\n",
    "If we think of our data as being fixed, then our loss function tells us how good or bad\n",
    "any particular model parameters are. This means we can use gradient descent to find\n",
    "the model parameters that *make the loss as small as possible*.\n",
    "\n",
    "Let’s look at a simple\n",
    "example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x ranges from -50 to 49, y is always 20 * x + 5\n",
    "inputs = [(x, 20 * x + 5) for x in range(-50, 50)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case we know the parameters of the linear relationship between `x` and `y`, but\n",
    "imagine we’d like to learn them from the data. We’ll use gradient descent to find the\n",
    "slope and intercept that minimize the average squared error.\n",
    "\n",
    "We’ll start off with a function that determines the gradient based on the error from a\n",
    "single data point.\n",
    "\n",
    "$$ y = mx + b + \\epsilon = \\hat{y} + \\epsilon$$\n",
    "\n",
    "$$ \\epsilon = y - \\hat{y} $$\n",
    "\n",
    "We then need a function that takes in our parameters as input and returns the squared error as output and we would be interested in minimizing this function. Let's call this function $l$.\n",
    "\n",
    "$$ l(m, b) = \\epsilon^2 = (y - \\hat{y})^2 = (y - mx - b)^2 $$\n",
    "\n",
    "Now let's take the gradient of $l$.\n",
    "\n",
    "$$ \\frac{\\delta l}{\\delta m} = 2 * (y - mx - b) * (-x) = -2 * \\epsilon * x $$\n",
    "\n",
    "$$ \\frac{\\delta l}{\\delta b} = 2 * (y - mx - b) * (-1) = -2 * \\epsilon$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_gradient(x: float, y: float, theta: Vector) -> Vector:\n",
    "    slope, intercept = theta\n",
    "    predicted = slope * x + intercept      # The prediction of the model.\n",
    "    error = y - predicted                  # error is (actual - predicted)\n",
    "    squared_error = error ** 2             # We'll minimize squared error\n",
    "    grad = [-2 * error * x, -2 * error]    # using its gradient.\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, that computation was for a single data point. For the whole dataset we’ll look at\n",
    "the *mean squared error*. And the gradient of the mean squared error is just the mean\n",
    "of the individual gradients.\n",
    "\n",
    "So, here’s what we’re going to do:\n",
    "1. Start with a random value for theta.\n",
    "2. Compute the mean of the gradients.\n",
    "3. Adjust theta in the direction of negative gradient (theta = theta - learning_rate * gradient)\n",
    "4. Repeat.\n",
    "\n",
    "After a lot of epochs (what we call each pass through the dataset), we should learn\n",
    "something like the correct parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with random values for slope and intercept.\n",
    "theta = [random.uniform(-1, 1), random.uniform(-1, 1)]\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "for epoch in range(5000):\n",
    "    # Compute the mean of the gradients\n",
    "    grad = np.mean([linear_gradient(x, y, theta) for x, y in inputs], axis = 0)\n",
    "    # Take a step in that direction of negative gradient to minimize\n",
    "    theta = gradient_step(theta, grad, -learning_rate)\n",
    "    print(epoch, theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slope, intercept = theta\n",
    "assert 19.9 < slope < 20.1,   \"slope should be about 20\"\n",
    "assert 4.9 < intercept < 5.1, \"intercept should be about 5\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the procedure is extremely sensitive to your choice the the `learning_rate` parameter.\n",
    "- too small, the process can be really slow\n",
    "- too big, the procedure may encounter numerical difficulties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minibatch and Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One drawback of the preceding approach is that we had to evaluate the gradients on\n",
    "the entire dataset before we could take a gradient step and update our parameters.\n",
    "In\n",
    "this case it was fine, because our dataset was only 100 pairs and the gradient computation\n",
    "was cheap.\n",
    "\n",
    "Your models, however, will frequently have large datasets and expensive gradient\n",
    "computations. In that case you’ll want to take gradient steps more often.\n",
    "\n",
    "We can do this using a technique called *minibatch gradient descent*, in which we compute\n",
    "the gradient (and take a gradient step) based on a “minibatch” sampled from the\n",
    "larger dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypeVar, List, Iterator\n",
    "\n",
    "T = TypeVar('T')  # this allows us to type \"generic\" functions\n",
    "\n",
    "def minibatches(dataset: List[T],\n",
    "                batch_size: int,\n",
    "                shuffle: bool = True) -> Iterator[List[T]]:\n",
    "    \"\"\"Generates `batch_size`-sized minibatches from the dataset\"\"\"\n",
    "    # Start indexes 0, batch_size, 2 * batch_size, ...\n",
    "    batch_starts = [start for start in range(0, len(dataset), batch_size)]\n",
    "\n",
    "    if shuffle: random.shuffle(batch_starts)  # shuffle the batches\n",
    "\n",
    "    for start in batch_starts:\n",
    "        end = start + batch_size\n",
    "        yield dataset[start:end]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: The `TypeVar(T)` allows us to create a “generic” function. It says that\n",
    "our dataset can be a list of any single type—`str`s, `int`s, `list`s,\n",
    "whatever—but whatever that type is, the outputs will be batches of\n",
    "it.\n",
    "\n",
    "Now we can solve our problem again using minibatches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minibatch gradient descent example\n",
    "\n",
    "theta = [random.uniform(-1, 1), random.uniform(-1, 1)]\n",
    "\n",
    "for epoch in range(1000):\n",
    "    for batch in minibatches(inputs, batch_size=20):\n",
    "        grad = np.mean([linear_gradient(x, y, theta) for x, y in batch], axis = 0)\n",
    "        theta = gradient_step(theta, grad, -learning_rate)\n",
    "    print(epoch, theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slope, intercept = theta\n",
    "assert 19.9 < slope < 20.1,   \"slope should be about 20\"\n",
    "assert 4.9 < intercept < 5.1, \"intercept should be about 5\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another variation is *stochastic gradient descent*, in which you take gradient steps\n",
    "based on one training example at a time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stochastic gradient descent example\n",
    "\n",
    "theta = [random.uniform(-1, 1), random.uniform(-1, 1)]\n",
    "\n",
    "for epoch in range(100):\n",
    "    for x, y in inputs:\n",
    "        grad = linear_gradient(x, y, theta)\n",
    "        theta = gradient_step(theta, grad, -learning_rate)\n",
    "    print(epoch, theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slope, intercept = theta\n",
    "assert 19.9 < slope < 20.1,   \"slope should be about 20\"\n",
    "assert 4.9 < intercept < 5.1, \"intercept should be about 5\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On this problem, stochastic gradient descent finds the optimal parameters in a much\n",
    "smaller number of epochs. But there are always tradeoffs.\n",
    "\n",
    "Basing gradient steps on\n",
    "small minibatches (or on single data points) allows you to take more of them, but the\n",
    "gradient for a single point might lie in a very different direction from the gradient for\n",
    "the dataset as a whole.\n",
    "\n",
    "Note: The terminology for the various flavors of gradient descent is not\n",
    "uniform. The “compute the gradient for the whole dataset”\n",
    "approach is often called *batch gradient descent*, and some people\n",
    "say *stochastic gradient descent* when referring to the minibatch version\n",
    "(of which the one-point-at-a-time version is a special case)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
