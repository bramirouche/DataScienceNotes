{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Deep learning* originally referred to the application of “deep” neural networks (that is,\n",
    "networks with more than one hidden layer), although in practice the term now\n",
    "encompasses a wide variety of neural architectures (including the “simple” neural\n",
    "networks we developed last week).\n",
    "\n",
    "we’ll continue to build on our previous work and look at a wider variety of neural\n",
    "networks. To do so, we’ll introduce a number of abstractions that allow us to think\n",
    "about neural networks in a more general way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previously, we made a distinction between vectors (one-dimensional arrays) and\n",
    "matrices (two-dimensional arrays). When we start working with more complicated\n",
    "neural networks, we’ll need to use higher-dimensional arrays as well.\n",
    "\n",
    "In many neural network libraries, n-dimensional arrays are referred to as tensors,\n",
    "which is what we’ll call them too.\n",
    "\n",
    "Many packages do implement a full-featured\n",
    "`Tensor` class that overloads Python’s arithmetic operators and could handle a variety\n",
    "of other operations. Here we’ll cheat and say that a `Tensor` is just a `list`.\n",
    "- This is true in one direction—all of our vectors and matrices and higher-dimensional analogues are lists (of lists).\n",
    "- It is certainly not true in the other direction—most Python `lists` are not *n*-dimensional arrays in our sense.\n",
    "\n",
    "Ideally we’d like to do something like:\n",
    "\n",
    "```python\n",
    "# A Tensor is either a float, or a List of Tensors\n",
    "Tensor = Union[float, List[Tensor]]\n",
    "```\n",
    "\n",
    "However, Python won’t let you define recursive types like that. And\n",
    "even if it did that definition is still not right, as it allows for bad\n",
    "“tensors” like:\n",
    "```python\n",
    "[[1.0, 2.0],\n",
    " [3.0]]\n",
    "```\n",
    "whose rows have different sizes, which makes it not an *n*-dimensional\n",
    "array that we can work with.\n",
    "\n",
    "So, as mentioned earlier, we’ll just cheat:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tensor = list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we’ll write a helper function to find a tensor’s *shape*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "def shape(tensor: Tensor) -> List[int]:\n",
    "    sizes: List[int] = []\n",
    "    while isinstance(tensor, list):\n",
    "        sizes.append(len(tensor))\n",
    "        tensor = tensor[0]\n",
    "    return sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(shape([1, 2, 3])) # [3]\n",
    "print(shape([[1, 2],\n",
    "             [3, 4],\n",
    "             [5, 6]])) # [3, 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because tensors can have any number of dimensions, we’ll typically need to work\n",
    "with them recursively. We’ll do one thing in the one-dimensional case and recurse in\n",
    "the higher-dimensional case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_1d(tensor: Tensor) -> bool:\n",
    "    \"\"\"\n",
    "    If tensor[0] is a list, it's a higher-order tensor.\n",
    "    Otherwise, tensor is 1-dimensonal (that is, a vector).\n",
    "    \"\"\"\n",
    "    return not isinstance(tensor[0], list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(is_1d([1, 2, 3]))\n",
    "print(is_1d([[1, 2],\n",
    "             [3, 4]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which we can use to write a recursive *tensor_sum* function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor_sum(tensor: Tensor) -> float:\n",
    "    \"\"\"Sums up all the values in the tensor\"\"\"\n",
    "    if is_1d(tensor):\n",
    "        return sum(tensor)  # just a list of floats, use Python sum\n",
    "    else:\n",
    "        return sum(tensor_sum(tensor_i)      # Call tensor_sum on each row\n",
    "                   for tensor_i in tensor)   # and sum up those results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tensor_sum([1, 2, 3])) # 6\n",
    "print(tensor_sum([[1, 2],\n",
    "                  [3, 4]])) # 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you’re not used to thinking recursively, you should ponder this until it makes sense,\n",
    "because we’ll use the same logic throughout this chapter. However, we’ll create a couple\n",
    "of helper functions so that we don’t have to rewrite this logic everywhere. The first\n",
    "applies a function elementwise to a single tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "\n",
    "# here f is a type of function that takes a single float as argument and returns a float\n",
    "def tensor_apply(f: Callable[[float], float], tensor: Tensor) -> Tensor:\n",
    "    \"\"\"Applies f elementwise\"\"\"\n",
    "    if is_1d(tensor):\n",
    "        return [f(x) for x in tensor]\n",
    "    else:\n",
    "        return [tensor_apply(f, tensor_i) for tensor_i in tensor]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tensor_apply(lambda x: x + 1, [1, 2, 3])) # [2, 3, 4]\n",
    "print(tensor_apply(lambda x: 2 * x, [[1, 2], [3, 4]])) # [[2, 4], [6, 8]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use this to write a function that creates a zero tensor with the same shape as a\n",
    "given tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zeros_like(tensor: Tensor) -> Tensor:\n",
    "    return tensor_apply(lambda _: 0.0, tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(zeros_like([1, 2, 3])) # [0, 0, 0]\n",
    "print(zeros_like([[1, 2], [3, 4]])) # [[0, 0], [0, 0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’ll also need to apply a function to corresponding elements from two tensors\n",
    "(which had better be the exact same shape, although we won’t check that):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor_combine(f: Callable[[float, float], float],\n",
    "                   t1: Tensor,\n",
    "                   t2: Tensor) -> Tensor:\n",
    "    \"\"\"Applies f to corresponding elements of t1 and t2\"\"\"\n",
    "    if is_1d(t1):\n",
    "        return [f(x, y) for x, y in zip(t1, t2)]\n",
    "    else:\n",
    "        return [tensor_combine(f, t1_i, t2_i)\n",
    "                for t1_i, t2_i in zip(t1, t2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "print(tensor_combine(operator.add, [1, 2, 3], [4, 5, 6])) # [5, 7, 9]\n",
    "print(tensor_combine(operator.mul, [1, 2, 3], [4, 5, 6])) # [4, 10, 18]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Layer Abstraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last week we built a simple neural net that allowed us to stack two layers\n",
    "of neurons, each of which computed `sigmoid(dot(weights, inputs))`.\n",
    "\n",
    "Although that’s perhaps an idealized representation of what an actual neuron does, in\n",
    "practice we’d like to allow a wider variety of things.\n",
    "- Perhaps we’d like the neurons to remember something about their previous inputs. \n",
    "\n",
    "- Perhaps we’d like to use a different activation function than sigmoid. \n",
    "\n",
    "- And frequently we’d like to use more than two layers. (Our `feed_forward` function actually handled any number of layers, but our gradient computations did not.)\n",
    "\n",
    "In this chapter we’ll build machinery for implementing such a variety of neural networks.\n",
    "Our fundamental abstraction will be the `Layer`, something that knows how to\n",
    "apply some function to its inputs and that knows how to backpropagate gradients.\n",
    "\n",
    "One way of thinking about the neural networks we built last week is as a “linear”\n",
    "layer (performing dot product between inputs and weights), followed by a “sigmoid” layer (apply sigmoid function to the sum resulted from the dot product), then another linear layer and another sigmoid\n",
    "layer. We didn’t distinguish them in these terms, but doing so will allow us to experiment\n",
    "with much more general structures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterable, Tuple\n",
    "\n",
    "class Layer:\n",
    "    \"\"\"\n",
    "    Our neural networks will be composed of Layers, each of which\n",
    "    knows how to do some computation on its inputs in the \"forward\"\n",
    "    direction and propagate gradients in the \"backward\" direction.\n",
    "    \"\"\"\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Note the lack of types. We're not going to be prescriptive\n",
    "        about what kinds of inputs layers can take and what kinds\n",
    "        of outputs they can return.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def backward(self, gradient):\n",
    "        \"\"\"\n",
    "        Similarly, we're not going to be prescriptive about what the\n",
    "        gradient looks like. It's up to you the user to make sure\n",
    "        that you're doing things sensibly.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def params(self) -> Iterable[Tensor]:\n",
    "        \"\"\"\n",
    "        Returns the parameters of this layer. The default implementation\n",
    "        returns nothing, so that if you have a layer with no parameters\n",
    "        you don't have to implement this.\n",
    "        \"\"\"\n",
    "        return ()\n",
    "\n",
    "    def grads(self) -> Iterable[Tensor]:\n",
    "        \"\"\"\n",
    "        Returns the gradients, in the same order as params()\n",
    "        \"\"\"\n",
    "        return ()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `forward` and `backward` methods will have to be implemented in our concrete\n",
    "subclasses. Once we build a neural net, we’ll want to train it using gradient descent,\n",
    "which means we’ll want to update each parameter in the network using its gradient.\n",
    "Accordingly, we insist that each layer be able to tell us its parameters and gradients.\n",
    "\n",
    "Some layers (for example, a layer that applies *sigmoid* to each of its inputs) have no\n",
    "parameters to update, so we provide a default implementation that handles that case.\n",
    "\n",
    "Let’s look at that layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Week11M import sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid(Layer):\n",
    "    def forward(self, input: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Apply sigmoid to each element of the input tensor,\n",
    "        and save the results to use in backpropagation.\n",
    "        \"\"\"\n",
    "        self.sigmoids = tensor_apply(sigmoid, input)\n",
    "        return self.sigmoids\n",
    "\n",
    "    def backward(self, gradient: Tensor) -> Tensor:\n",
    "        return tensor_combine(lambda sig, grad: sig * (1 - sig) * grad,\n",
    "                              self.sigmoids,\n",
    "                              gradient)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a couple of things to notice here. One is that during the forward pass we\n",
    "saved the computed sigmoids so that we could use them later in the backward pass.\n",
    "Our layers will typically need to do this sort of thing.\n",
    "\n",
    "Second, you may be wondering where the `sig * (1 - sig) * grad` comes from.\n",
    "This is the chain rule from calculus and corresponds to the `output * (1 - output) * (output - target)` term in our previous neural networks.\n",
    "\n",
    "Finally, you can see how we were able to make use of the `tensor_apply` and the `tensor_combine` functions. Most of our layers will use these functions similarly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Linear Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The other piece we’ll need to duplicate the neural networks from last week is a “linear”\n",
    "layer that represents the `dot(weights, inputs)` part of the neurons.\n",
    "\n",
    "This layer will have parameters, which we’d like to initialize with random values.\n",
    "\n",
    "It turns out that the initial parameter values can make a huge difference in how\n",
    "quickly (and sometimes *whether*) the network trains. If weights are too big, they may\n",
    "produce large outputs in a range where the activation function has near-zero gradients.\n",
    "And parts of the network that have zero gradients necessarily can’t learn anything\n",
    "via gradient descent.\n",
    "\n",
    "Accordingly, we’ll implement three different schemes for randomly generating our\n",
    "weight tensors.\n",
    "1. The first is to choose each value from the random uniform distribution on [0, 1]—that is, as a `random.random()`.\n",
    "2. The second (and default) is to choose each value randomly from a standard normal distribution.\n",
    "3. And the third is to use *Xavier initialization*, where each weight is initialized with a random draw from a normal distribution with mean 0 and variance `2 / (num_inputs + num_outputs)`. It turns out this often works nicely for neural network weights. We’ll implement these with a `random_uniform` function and a `random_normal` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def random_uniform(*dims: int) -> Tensor:\n",
    "    if len(dims) == 1:\n",
    "        return [random.random() for _ in range(dims[0])]\n",
    "    else:\n",
    "        return [random_uniform(*dims[1:]) for _ in range(dims[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "\n",
    "def random_normal(*dims: int,\n",
    "                  mean: float = 0.0,\n",
    "                  variance: float = 1.0) -> Tensor:\n",
    "    if len(dims) == 1:\n",
    "        return [mean + variance * norm.ppf(random.random())\n",
    "                for _ in range(dims[0])]\n",
    "    else:\n",
    "        return [random_normal(*dims[1:], mean=mean, variance=variance)\n",
    "                for _ in range(dims[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ran1 = random_uniform(2, 3, 4)\n",
    "print(ran1)\n",
    "print(shape(ran1))\n",
    "\n",
    "ran2 = random_normal(5, 6, mean=10)\n",
    "print(ran2)\n",
    "print(shape(ran2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then wrap them all in a `random_tensor` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_tensor(*dims: int, init: str = 'normal') -> Tensor:\n",
    "    if init == 'normal':\n",
    "        return random_normal(*dims)\n",
    "    elif init == 'uniform':\n",
    "        return random_uniform(*dims)\n",
    "    elif init == 'xavier':\n",
    "        variance = len(dims) / sum(dims)\n",
    "        return random_normal(*dims, variance=variance)\n",
    "    else:\n",
    "        raise ValueError(f\"unknown init: {init}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can define our linear layer. We need to initialize it with the dimension of the\n",
    "inputs (which tells us how many weights each neuron needs), the dimension of the\n",
    "outputs (which tells us how many neurons we should have), and the initialization\n",
    "scheme we want:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import dot\n",
    "\n",
    "class Linear(Layer):\n",
    "    def __init__(self, input_dim: int, output_dim: int, init: str = 'xavier') -> None:\n",
    "        \"\"\"\n",
    "        A layer of output_dim neurons, each with input_dim weights\n",
    "        (and a bias).\n",
    "        \"\"\"\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        # self.w[o] is the weights for the o-th neuron\n",
    "        self.w = random_tensor(output_dim, input_dim, init=init)\n",
    "\n",
    "        # self.b[o] is the bias term for the o-th neuron\n",
    "        self.b = random_tensor(output_dim, init=init)\n",
    "\n",
    "    # The forward method is easy to implement. We’ll get one output per neuron, which\n",
    "    # we stick in a vector. And each neuron’s output is just the dot of its weights with the\n",
    "    # input, plus its bias:\n",
    "    def forward(self, input: Tensor) -> Tensor:\n",
    "        # Save the input to use in the backward pass.\n",
    "        self.input = input\n",
    "\n",
    "        # Return the vector of neuron outputs.\n",
    "        return [dot(input, self.w[o]) + self.b[o]\n",
    "                for o in range(self.output_dim)]\n",
    "\n",
    "    # The backward method is more involved and requires calculus\n",
    "    # Here the input parameter 'gradient' corresponds to the delta value in our earlier (optional) derivation\n",
    "    # of the backpropogation algorithm.\n",
    "    def backward(self, gradient: Tensor) -> Tensor:\n",
    "        \n",
    "        # computes the rate of change of the loss function with respect to the current layer's weights and biases\n",
    "        # for bias, the rate of change happens to be just delta\n",
    "        self.b_grad = gradient \n",
    "        # for weights, the rate of change is delta * input of the current layer\n",
    "        # refer to 'delta_2 * h' and 'delta_1 * x' in our (optional) backpropagation derivation \n",
    "        self.w_grad = [[self.input[i] * gradient[o]\n",
    "                        for i in range(self.input_dim)]\n",
    "                       for o in range(self.output_dim)]\n",
    "        \n",
    "        # computes the new delta to be backpropagated into previous layers\n",
    "        # refer to 'delta_2 * w2' in our (optional) backpropagation derivation\n",
    "        return [sum(self.w[o][i] * gradient[o] for o in range(self.output_dim))\n",
    "                for i in range(self.input_dim)]\n",
    "\n",
    "    def params(self) -> Iterable[Tensor]:\n",
    "        return [self.w, self.b]\n",
    "\n",
    "    def grads(self) -> Iterable[Tensor]:\n",
    "        return [self.w_grad, self.b_grad]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks as a Sequence of Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’d like to think of neural networks as sequences of layers, so let’s come up with a\n",
    "way to combine multiple layers into one. The resulting neural network is itself a layer,\n",
    "and it implements the `Layer` methods in the obvious ways:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "class Sequential(Layer):\n",
    "    \"\"\"\n",
    "    A layer consisting of a sequence of other layers.\n",
    "    It's up to you to make sure that the output of each layer\n",
    "    makes sense as the input to the next layer.\n",
    "    \"\"\"\n",
    "    def __init__(self, layers: List[Layer]) -> None:\n",
    "        self.layers = layers\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"Just forward the input through the layers in order.\"\"\"\n",
    "        for layer in self.layers:\n",
    "            input = layer.forward(input)\n",
    "        return input\n",
    "\n",
    "    def backward(self, gradient):\n",
    "        \"\"\"Just backpropagate the gradient through the layers in reverse.\"\"\"\n",
    "        for layer in reversed(self.layers):\n",
    "            gradient = layer.backward(gradient)\n",
    "        return gradient\n",
    "\n",
    "    def params(self) -> Iterable[Tensor]:\n",
    "        \"\"\"Just return the params from each layer.\"\"\"\n",
    "        return (param for layer in self.layers for param in layer.params())\n",
    "\n",
    "    def grads(self) -> Iterable[Tensor]:\n",
    "        \"\"\"Just return the grads from each layer.\"\"\"\n",
    "        return (grad for layer in self.layers for grad in layer.grads())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we could represent the neural network we used for XOR as:\n",
    "\n",
    "```python\n",
    "xor_net = Sequential([\n",
    "    Linear(input_dim=2, output_dim=2),\n",
    "    Sigmoid(),\n",
    "    Linear(input_dim=2, output_dim=1),\n",
    "    Sigmoid()\n",
    "])\n",
    "```\n",
    "But we still need a little more machinery to train it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
