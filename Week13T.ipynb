{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning (Continued)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3]\n",
      "[3, 2]\n",
      "True\n",
      "False\n",
      "6\n",
      "10\n",
      "[2, 3, 4]\n",
      "[[2, 4], [6, 8]]\n",
      "[0.0, 0.0, 0.0]\n",
      "[[0.0, 0.0], [0.0, 0.0]]\n",
      "[5, 7, 9]\n",
      "[4, 10, 18]\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'Week11M'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# When you import it, you may want to comment out the slow neural network training code in the Python script file\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mWeek12R\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[1;32m~\\Documents\\CSCI 3360\\Week12R.py:9\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#!/usr/bin/env python\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# coding: utf-8\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# # Deep Learning (Continued)\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# In[1]:\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mWeek12MT\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# ## Loss and Optimization\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Previously we wrote out individual loss functions and gradient functions for our\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     18\u001b[0m \n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# In[2]:\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mLoss\u001b[39;00m:\n",
      "File \u001b[1;32m~\\Documents\\CSCI 3360\\Week12MT.py:269\u001b[0m\n\u001b[0;32m    253\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m ()\n\u001b[0;32m    256\u001b[0m \u001b[38;5;66;03m# The `forward` and `backward` methods will have to be implemented in our concrete\u001b[39;00m\n\u001b[0;32m    257\u001b[0m \u001b[38;5;66;03m# subclasses. Once we build a neural net, we’ll want to train it using gradient descent,\u001b[39;00m\n\u001b[0;32m    258\u001b[0m \u001b[38;5;66;03m# which means we’ll want to update each parameter in the network using its gradient.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    265\u001b[0m \n\u001b[0;32m    266\u001b[0m \u001b[38;5;66;03m# In[ ]:\u001b[39;00m\n\u001b[1;32m--> 269\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mWeek11M\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sigmoid\n\u001b[0;32m    272\u001b[0m \u001b[38;5;66;03m# In[ ]:\u001b[39;00m\n\u001b[0;32m    275\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mSigmoid\u001b[39;00m(Layer):\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'Week11M'"
     ]
    }
   ],
   "source": [
    "# When you import it, you may want to comment out the slow neural network training code in the Python script file\n",
    "from Week12R import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like most machine learning models, neural networks are prone to overfitting to their\n",
    "training data. We’ve previously seen ways to mitigate this; for example, using regularization in regression by penalizing large weights and that helped prevent overfitting.\n",
    "\n",
    "A common way of regularizing neural networks is using *dropout*. At training time, we\n",
    "randomly turn off each neuron (that is, replace its output with 0) with some fixed\n",
    "probability. This means that the network can’t learn to depend on any individual neuron,\n",
    "which seems to help with overfitting.\n",
    "\n",
    "At evaluation time, we don’t want to dropout any neurons, so a `Dropout` layer will\n",
    "need to know whether it’s training or not. In addition, at training time a `Dropout`\n",
    "layer only passes on some random fraction of its input. To make its output comparable\n",
    "during evaluation, we’ll scale down the outputs (uniformly) using that same fraction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dropout(Layer):\n",
    "    def __init__(self, p: float) -> None:\n",
    "        self.p = p\n",
    "        self.train = True\n",
    "\n",
    "    def forward(self, input: Tensor) -> Tensor:\n",
    "        if self.train:\n",
    "            # Create a mask of 0s and 1s shaped like the input\n",
    "            # using the specified probability.\n",
    "            self.mask = tensor_apply(\n",
    "                lambda _: 0 if random.random() < self.p else 1,\n",
    "                input)\n",
    "            # Multiply by the mask to dropout inputs.\n",
    "            return tensor_combine(operator.mul, input, self.mask)\n",
    "        else:\n",
    "            # During evaluation just scale down the outputs uniformly.\n",
    "            return tensor_apply(lambda x: x * (1 - self.p), input)\n",
    "\n",
    "    def backward(self, gradient: Tensor) -> Tensor:\n",
    "        if self.train:\n",
    "            # Only propagate the gradients where mask == 1\n",
    "            return tensor_combine(operator.mul, gradient, self.mask)\n",
    "        else:\n",
    "            raise RuntimeError(\"don't call backward when not in train mode\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[MNIST](http://yann.lecun.com/exdb/mnist/) is a dataset of handwritten digits that everyone uses to learn deep learning. It is available in a somewhat tricky binary format, so we’ll install the mnist library to work with it (note that this package does *not* come pre-installed with anaconda)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then we can load the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mnist\n",
    "\n",
    "# This will download the data, change this to where you want it.\n",
    "# You may need to manually create this directory if it doesn't already exists\n",
    "mnist.temporary_dir = lambda: './data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each of these functions first downloads the data and returns a numpy array.\n",
    "# We call .tolist() because our \"tensors\" are just lists.\n",
    "train_images = mnist.train_images().tolist()\n",
    "train_labels = mnist.train_labels().tolist()\n",
    "\n",
    "print(shape(train_images)) # [60000, 28, 28]\n",
    "print(shape(train_labels)) # [60000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s plot the first 100 training images to see what they look like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (12,12)\n",
    "\n",
    "fig, ax = plt.subplots(10, 10)\n",
    "\n",
    "for i in range(10):\n",
    "    for j in range(10):\n",
    "        # Plot each image in black and white and hide the axes.\n",
    "        ax[i][j].imshow(train_images[10 * i + j], cmap='Greys')\n",
    "        ax[i][j].xaxis.set_visible(False)\n",
    "        ax[i][j].yaxis.set_visible(False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that indeed they look like handwritten digits.\n",
    "\n",
    "We also need to load the test images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the MNIST test data\n",
    "\n",
    "test_images = mnist.test_images().tolist()\n",
    "test_labels = mnist.test_labels().tolist()\n",
    "\n",
    "print(shape(test_images)) # [10000, 28, 28]\n",
    "print(shape(test_labels)) # [10000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each image is 28 × 28 pixels, but our linear layers can only deal with one-dimensional\n",
    "inputs, so we’ll just flatten them (and also divide by 256 to get them between 0 and 1).\n",
    "\n",
    "In addition, our neural net will train better if our inputs are 0 on average, so we’ll\n",
    "subtract out the average value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recenter the images\n",
    "\n",
    "# Compute the average pixel value\n",
    "avg = tensor_sum(train_images) / 60000 / 28 / 28\n",
    "\n",
    "# Recenter, rescale, and flatten\n",
    "train_images = [[(pixel - avg) / 256 for row in image for pixel in row]\n",
    "                for image in train_images]\n",
    "test_images = [[(pixel - avg) / 256 for row in image for pixel in row]\n",
    "               for image in test_images]\n",
    "\n",
    "print(shape(train_images)) # [60000, 784]\n",
    "print(shape(test_images))  # [10000, 784]\n",
    "\n",
    "# After centering, average pixel should be very close to 0\n",
    "print(tensor_sum(train_images))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also want to one-hot-encode the targets, since we have 10 outputs. First let’s write\n",
    "a `one_hot_encode` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(i: int, num_labels: int = 10) -> List[float]:\n",
    "    return [1.0 if j == i else 0.0 for j in range(num_labels)]\n",
    "\n",
    "print(one_hot_encode(3)) # [0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
    "print(one_hot_encode(2, num_labels=5)) # [0, 0, 1, 0, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and then apply it to our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode the test data\n",
    "\n",
    "train_labels = [one_hot_encode(label) for label in train_labels]\n",
    "test_labels = [one_hot_encode(label) for label in test_labels]\n",
    "\n",
    "print(shape(train_labels)) # [60000, 10]\n",
    "print(shape(test_labels))  # [10000, 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the strengths of our abstractions is that we can use the same training/evaluation\n",
    "loop with a variety of models. So let’s write that first. We’ll pass it our model, the\n",
    "data, a loss function, and (if we’re training) an optimizer.\n",
    "\n",
    "It will make a pass through our data, track performance, and (if we passed in an optimizer)\n",
    "update our parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "\n",
    "# Training loop\n",
    "\n",
    "def loop(model: Layer,\n",
    "         images: List[Tensor],\n",
    "         labels: List[Tensor],\n",
    "         loss: Loss,\n",
    "         optimizer: Optimizer = None) -> None:\n",
    "    correct = 0         # Track number of correct predictions.\n",
    "    total_loss = 0.0    # Track total loss.\n",
    "\n",
    "    with tqdm.trange(len(images)) as t:\n",
    "        for i in t:\n",
    "            predicted = model.forward(images[i])             # Predict.\n",
    "            if argmax(predicted) == argmax(labels[i]):       # Check for\n",
    "                correct += 1                                 # correctness.\n",
    "            total_loss += loss.loss(predicted, labels[i])    # Compute loss.\n",
    "\n",
    "            # If we're training, backpropagate gradient and update weights.\n",
    "            if optimizer is not None:\n",
    "                gradient = loss.gradient(predicted, labels[i])\n",
    "                model.backward(gradient)\n",
    "                optimizer.step(model)\n",
    "\n",
    "            # And update our metrics in the progress bar.\n",
    "            avg_loss = total_loss / (i + 1)\n",
    "            acc = correct / (i + 1)\n",
    "            t.set_description(f\"mnist loss: {avg_loss:.3f} acc: {acc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a baseline, we can use our deep learning library to train a simple model which consists of just a single linear layer followed by a softmax. This model (in essence) just looks for 10 linear functions such that if the input represents, say, a 5,\n",
    "then the 5th linear function produces the largest output.\n",
    "\n",
    "One pass through our 60,000 training examples should be enough to learn the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "\n",
    "# baseline model is just a linear layer followed by softmax\n",
    "model = Linear(784, 10)\n",
    "loss = SoftmaxCrossEntropy()\n",
    "\n",
    "# This optimizer seems to work\n",
    "optimizer = Momentum(learning_rate=0.01, momentum=0.99)\n",
    "\n",
    "# Train on the training data\n",
    "loop(model, train_images, train_labels, loss, optimizer)\n",
    "\n",
    "# Test on the test data (no optimizer means just evaluate)\n",
    "loop(model, test_images, test_labels, loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This should get about 89% accuracy. Let’s see if we can do better with a deep neural network.\n",
    "We’ll use two hidden layers, the first with 30 neurons, and the second with 10\n",
    "neurons. And we’ll use our `Tanh` activation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A deep neural network for MNIST\n",
    "\n",
    "random.seed(0)\n",
    "\n",
    "# Name them so we can turn train on and off\n",
    "dropout1 = Dropout(0.1)\n",
    "dropout2 = Dropout(0.1)\n",
    "\n",
    "model = Sequential([\n",
    "    Linear(784, 30),  # Hidden layer 1: size 30\n",
    "    dropout1,\n",
    "    Tanh(),\n",
    "    Linear(30, 10),   # Hidden layer 2: size 10\n",
    "    dropout2,\n",
    "    Tanh(),\n",
    "    Linear(10, 10)    # Output layer: size 10\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can just use the same training loop!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the deep model for MNIST\n",
    "\n",
    "optimizer = Momentum(learning_rate=0.01, momentum=0.99)\n",
    "loss = SoftmaxCrossEntropy()\n",
    "\n",
    "# Enable dropout and train (this will take a while)\n",
    "dropout1.train = dropout2.train = True\n",
    "loop(model, train_images, train_labels, loss, optimizer)\n",
    "\n",
    "# Disable dropout and evaluate\n",
    "dropout1.train = dropout2.train = False\n",
    "loop(model, test_images, test_labels, loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our deep model gets better than 92% accuracy on the test set, which is a nice\n",
    "improvement from the simple baseline model.\n",
    "\n",
    "The [MNIST website](http://yann.lecun.com/exdb/mnist/) describes a variety of models that outperform these. Many of\n",
    "them could be implemented using the machinery we’ve developed so far but would\n",
    "take an extremely long time to train in our lists-as-tensors framework. Some of the\n",
    "best models involve convolutional layers, which are important but unfortunately quite\n",
    "out of scope for an introductory course on data science."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and Loading Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These models take a long time to train, so it would be nice if we could save them so\n",
    "that we don’t have to train them every time. Luckily, we can use the `json` module to\n",
    "easily serialize model weights to a file.\n",
    "\n",
    "For saving, we can use `Layer.params` to collect the weights, stick them in a list, and\n",
    "use `json.dump` to save that list to a file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def save_weights(model: Layer, filename: str) -> None:\n",
    "    weights = list(model.params())\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(weights, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the weights back is only a little more work. We just use `json.load` to get the\n",
    "list of weights back from the file and slice assignment to set the weights of our model.\n",
    "\n",
    "(In particular, this means that we have to instantiate the model ourselves and then\n",
    "load the weights. An alternative approach would be to also save some representation\n",
    "of the model architecture and use that to instantiate the model. That’s not a terrible\n",
    "idea, but it would require a lot more code and changes to all our Layers, so we’ll stick\n",
    "with the simpler way.)\n",
    "\n",
    "Before we load the weights, we’d like to check that they have the same shapes as the\n",
    "model params we’re loading them into. (This is a safeguard against, for example, trying\n",
    "to load the weights for a saved deep network into a shallow network, or similar\n",
    "issues.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_weights(model: Layer, filename: str) -> None:\n",
    "    with open(filename) as f:\n",
    "        weights = json.load(f)\n",
    "\n",
    "    # Check for consistency\n",
    "    assert all(shape(param) == shape(weight)\n",
    "               for param, weight in zip(model.params(), weights))\n",
    "\n",
    "    # Then load using slice assignment:\n",
    "    for param, weight in zip(model.params(), weights):\n",
    "        param[:] = weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JSON stores your data as text, which makes it an extremely inefficient\n",
    "representation. In real applications you’d probably use the\n",
    "`pickle` serialization library, which serializes things to a more efficient\n",
    "binary format. Here I decided to keep it simple and human-readable."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
