{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An *artificial neural network* (or neural network for short) is a predictive model motivated\n",
    "by the way the brain operates. Think of the brain as a collection of neurons\n",
    "wired together. Each neuron looks at the outputs of the other neurons that feed into\n",
    "it, does a calculation, and then either fires (if the calculation exceeds some threshold)\n",
    "or doesn’t (if it doesn’t).\n",
    "\n",
    "Accordingly, artificial neural networks consist of artificial neurons, which perform\n",
    "similar calculations over their inputs. Neural networks can solve a wide variety of\n",
    "problems like handwriting recognition and face detection, and they are used heavily\n",
    "in deep learning, one of the trendiest subfields of data science.\n",
    "\n",
    "However, most neural\n",
    "networks are “black boxes”—inspecting their details doesn’t give you much understanding\n",
    "of how they’re solving a problem. And large neural networks can be difficult\n",
    "to train. For most problems you’ll encounter as a budding data scientist, they’re probably\n",
    "not the right choice. Nevertheless, it is important to introduce this model, because if you\n",
    "continue on the path of data science, you will have to dig into it sooner or later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptrons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty much the simplest neural network is the *perceptron*, which approximates a single\n",
    "neuron with n binary inputs. It computes a weighted sum of its inputs and “fires”\n",
    "if that weighted sum is 0 or greater:\n",
    "\n",
    "![Perceptron](perceptron.png)\n",
    "\n",
    "Image Credit: https://towardsdatascience.com/the-perceptron-3af34c84838c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import List\n",
    "Vector = List[float]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_function(x: float) -> float:\n",
    "    return 1.0 if x >= 0 else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perceptron_output(weights: Vector, bias: float, x: Vector) -> float:\n",
    "    \"\"\"Returns 1 if the perceptron 'fires', 0 if not\"\"\"\n",
    "    calculation = np.dot(weights, x) + bias\n",
    "    return step_function(calculation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For those of you who are familiar with advanced geometry, the perceptron is simply distinguishing between the half-spaces separated by the\n",
    "hyperplane:\n",
    "\n",
    "```python\n",
    "dot(weights, x) + bias == 0\n",
    "```\n",
    "\n",
    "With properly chosen weights, perceptrons can solve a number of simple problems. For example, we can create an AND gate (which returns 1 if both its\n",
    "inputs are 1 but returns 0 if one of its inputs is 0) with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 AND 1 = 1.0\n",
      "0 AND 1 = 0.0\n",
      "1 AND 0 = 0.0\n",
      "0 AND 0 = 0.0\n"
     ]
    }
   ],
   "source": [
    "and_weights = [2, 2]\n",
    "and_bias = -3.\n",
    "\n",
    "print(f\"1 AND 1 = {perceptron_output(and_weights, and_bias, [1, 1])}\")\n",
    "print(f\"0 AND 1 = {perceptron_output(and_weights, and_bias, [0, 1])}\")\n",
    "print(f\"1 AND 0 = {perceptron_output(and_weights, and_bias, [1, 0])}\")\n",
    "print(f\"0 AND 0 = {perceptron_output(and_weights, and_bias, [0, 0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If both inputs are 1, the `calculation` equals 2 + 2 – 3 = 1, and the output is 1. If only\n",
    "one of the inputs is 1, the `calculation` equals 2 + 0 – 3 = –1, and the output is 0. And\n",
    "if both of the inputs are 0, the `calculation` equals –3, and the output is 0.\n",
    "\n",
    "Using similar reasoning, we could build an OR gate with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 OR 1 = 1.0\n",
      "0 OR 1 = 1.0\n",
      "1 OR 0 = 1.0\n",
      "0 OR 0 = 0.0\n"
     ]
    }
   ],
   "source": [
    "or_weights = [2, 2]\n",
    "or_bias = -1.\n",
    "\n",
    "print(f\"1 OR 1 = {perceptron_output(or_weights, or_bias, [1, 1])}\")\n",
    "print(f\"0 OR 1 = {perceptron_output(or_weights, or_bias, [0, 1])}\")\n",
    "print(f\"1 OR 0 = {perceptron_output(or_weights, or_bias, [1, 0])}\")\n",
    "print(f\"0 OR 0 = {perceptron_output(or_weights, or_bias, [0, 0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Decision space for a two-input perceptron](decision_space.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could also build a NOT gate (which has one input and converts 1 to 0 and 0 to 1)\n",
    "with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOT 0 = 1.0\n",
      "NOT 1 = 0.0\n"
     ]
    }
   ],
   "source": [
    "not_weights = [-2]\n",
    "not_bias = 1\n",
    "\n",
    "print(f\"NOT 0 = {perceptron_output(not_weights, not_bias, [0])}\")\n",
    "print(f\"NOT 1 = {perceptron_output(not_weights, not_bias, [1])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, there are some problems that simply can’t be solved by a single perceptron.\n",
    "For example, no matter how hard you try, you cannot use a perceptron to build an\n",
    "XOR gate that outputs 1 if exactly one of its inputs is 1 and 0 otherwise. This is where\n",
    "we start needing more complicated neural networks.\n",
    "\n",
    "Of course, you don’t need to approximate a neuron in order to build a logic gate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "and_gate = min\n",
    "or_gate = max\n",
    "xor_gate = lambda x, y: 0 if x == y else 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like real neurons, artificial neurons start getting (and complicated) more interesting when you start\n",
    "connecting them together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feed-Forward Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The topology of the brain is enormously complicated, so it’s common to approximate\n",
    "it with an idealized *feed-forward* neural network that consists of discrete layers of\n",
    "neurons, each connected to the next.\n",
    "\n",
    "This typically entails an input layer (which\n",
    "receives inputs and feeds them forward unchanged), one or more “hidden layers”\n",
    "(each of which consists of neurons that take the outputs of the previous layer, performs\n",
    "some calculation, and passes the result to the next layer), and an output layer\n",
    "(which produces the final outputs).\n",
    "\n",
    "![Feedforward Neural Networks](nn.jpg)\n",
    "\n",
    "Image Credit: https://www.learnopencv.com/understanding-feedforward-neural-networks/\n",
    "\n",
    "Just like in the perceptron, each (noninput) neuron has a weight corresponding to\n",
    "each of its inputs and a bias. To make our representation simpler, we’ll add the bias to\n",
    "the end of our weights vector and give each neuron a bias input that always equals 1 (this is similar to the way we represent the intercept in multiple linear regression with a column of 1's).\n",
    "\n",
    "As with the perceptron, for each neuron we’ll sum up the products of its inputs and\n",
    "its weights. But here, rather than outputting the `step_function` applied to that product,\n",
    "we’ll output a smooth approximation of it. Here we’ll use the `sigmoid` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def sigmoid(t: float) -> float:\n",
    "    return 1 / (1 + math.exp(-t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![sigmoid](sigmoid.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why use `sigmoid` instead of the simpler `step_function`? In order to train a neural\n",
    "network, we need to use calculus, and in order to use calculus, we need smooth functions (that are differentiable).\n",
    "`step_function` isn’t even continuous, and `sigmoid` is a good smooth approximation\n",
    "of it.\n",
    "\n",
    "We then calculate the output as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neuron_output(weights: Vector, inputs: Vector) -> float:\n",
    "    # weights includes the bias term, inputs includes a 1\n",
    "    return sigmoid(np.dot(weights, inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4700359482354282"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neuron_output((1.3, -0.6, 0.2),(0.2, 0.8, 0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given this function, we can represent a neuron simply as a vector of weights whose\n",
    "length is one more than the number of inputs to that neuron (because of the bias\n",
    "weight). Then we can represent a neural network as a list of (noninput) layers, where\n",
    "each layer is just a list of the neurons in that layer.\n",
    "\n",
    "That is, we’ll represent a neural network as a list (layers) of lists (neurons) of vectors\n",
    "(weights).\n",
    "\n",
    "Given such a representation, using the neural network is quite simple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feed_forward(neural_network: List[List[Vector]],\n",
    "                 input_vector: Vector) -> List[Vector]:\n",
    "    \"\"\"\n",
    "    Feeds the input vector through the neural network.\n",
    "    Returns the outputs of all layers (not just the last one).\n",
    "    \"\"\"\n",
    "    outputs: List[Vector] = []\n",
    "\n",
    "    for layer in neural_network:\n",
    "        input_with_bias = input_vector + [1]              # Add a constant.\n",
    "        output = [neuron_output(neuron, input_with_bias)  # Compute the output\n",
    "                  for neuron in layer]                    # for each neuron.\n",
    "        outputs.append(output)                            # Add to results.\n",
    "\n",
    "        # Then the input to the next layer is the output of this one\n",
    "        input_vector = output\n",
    "\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we could build the XOR gate that we couldn’t build with a single perceptron. Here is one design:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "xor_network = [# hidden layer\n",
    "               [[20, 20, -30],      # 'and' neuron\n",
    "                [20, 20, -10]],     # 'or'  neuron\n",
    "               # output layer\n",
    "               [[-60, 60, -30]]]    # 2nd input AND (NOT 1st input) neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 XOR 1 = 9.383146683006828e-14\n",
      "0 XOR 1 = 0.9999999999999059\n",
      "1 XOR 0 = 0.9999999999999059\n",
      "0 XOR 0 = 9.38314668300676e-14\n"
     ]
    }
   ],
   "source": [
    "# feed_forward returns the outputs of all layers, so the [-1] gets the\n",
    "# final output, and the [0] gets the value out of the resulting vector\n",
    "print(f\"1 XOR 1 = {feed_forward(xor_network, [1, 1])[-1][0]}\")\n",
    "print(f\"0 XOR 1 = {feed_forward(xor_network, [0, 1])[-1][0]}\")\n",
    "print(f\"1 XOR 0 = {feed_forward(xor_network, [1, 0])[-1][0]}\")\n",
    "print(f\"0 XOR 0 = {feed_forward(xor_network, [0, 0])[-1][0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![XOR](xor.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One suggestive way of thinking about this is that the hidden layer is computing features\n",
    "of the input data (in this case “and” and “or”) and the output layer is combining\n",
    "those features in a way that generates the desired output."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
