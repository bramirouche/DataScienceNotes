{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The VP of Talent at your social network start-up has interviewed a number of job candidates from the\n",
    "site, with varying degrees of success. He’s collected a dataset consisting of several\n",
    "(qualitative) attributes of each candidate, as well as whether that candidate interviewed\n",
    "well or poorly. Could you, he asks, use this data to build a model identifying\n",
    "which candidates will interview well, so that he doesn’t have to waste time conducting\n",
    "interviews?\n",
    "This seems like a good fit for a decision tree, another predictive modeling tool in the\n",
    "data scientist’s kit.\n",
    "\n",
    "This seems like a good fit for a *decision tree*, another predictive modeling tool in the\n",
    "data scientist’s kit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What Is a Decision Tree?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A decision tree uses a tree structure to represent a number of possible decision paths\n",
    "and an outcome for each path.\n",
    "\n",
    "If you have ever played the game *Twenty Questions*, then you are familiar with decision\n",
    "trees. For example:\n",
    "- “I am thinking of an animal.”\n",
    "- “Does it have four legs?”\n",
    "- “No.”\n",
    "- “Does it fly?”\n",
    "- “Yes.”\n",
    "- “Does it appear on the back of a quarter coin?”\n",
    "- “Yes.”\n",
    "- “Is it an eagle?”\n",
    "- “Yes, it is!”\n",
    "\n",
    "This corresponds to the path: \"Does not have four legs\" -> \"Does fly\" -> \"On the quarter coin\" -> \"Eagle\"\n",
    "\n",
    "in a possible “guess the animal” decision tree. Below is an example of a (not so comprehensive) decision tree.\n",
    "\n",
    "![A \"guess the animal\" decision tree](decision_tree.jpg)\n",
    "\n",
    "Decision trees have a lot to recommend them. They’re very easy to understand and\n",
    "interpret, and the process by which they reach a prediction is completely transparent.\n",
    "Unlike the other models we’ve looked at so far, decision trees can easily handle a mix\n",
    "of numeric (e.g., number of legs) and categorical (e.g., fly/not fly)\n",
    "attributes.\n",
    "\n",
    "At the same time, finding an “optimal” decision tree for a set of training data is computationally\n",
    "a very hard problem. (We will get around this by trying to build a good enough\n",
    "tree rather than an optimal one, although for large datasets this can still be a\n",
    "lot of work.) More important, it is very easy (and very bad) to build decision trees\n",
    "that are overfitted to the training data, and that don’t generalize well to unseen data.\n",
    "We’ll look at ways to address this.\n",
    "\n",
    "Most people divide decision trees into classification trees (which produce categorical\n",
    "outputs) and regression trees (which produce numeric outputs). Here, we’ll\n",
    "focus on classification trees, and we’ll work through the ID3 algorithm for learning a\n",
    "decision tree from a set of labeled data, which should help us understand how decision\n",
    "trees actually work.\n",
    "\n",
    "To make things simple, we’ll restrict ourselves to problems\n",
    "with binary outputs like “Should I hire this candidate?” or “Should I show this website\n",
    "visitor advertisement A or advertisement B?” or “Will eating this food I found in\n",
    "the office fridge make me sick?”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to build a decision tree, we will need to decide what questions to ask and in\n",
    "what order. At each stage of the tree there are some possibilities we’ve eliminated and\n",
    "some that we haven’t.\n",
    "\n",
    "After learning that an animal doesn’t have more than five legs,\n",
    "we’ve eliminated the possibility that it’s a grasshopper. We haven’t eliminated the possibility\n",
    "that it’s a duck.\n",
    "\n",
    "Each possible question *partitions* the remaining possibilities\n",
    "according to its answer.\n",
    "\n",
    "Ideally, we’d like to choose questions whose answers give a lot of information about\n",
    "what our tree should predict.\n",
    "- If there’s a single yes/no question for which “yes” answers always correspond to True outputs and “no” answers to False outputs (or vice versa), this would be an awesome question to pick.\n",
    "- Conversely, a yes/no question for which neither answer gives you much new information about what the prediction should be is probably not a good choice.\n",
    "\n",
    "We capture this notion of “how much information” with *entropy*. You have probably\n",
    "heard this term used to mean disorder. We use it to represent the uncertainty associated\n",
    "with data.\n",
    "\n",
    "Imagine that we have a set $S$ of data, each member of which is labeled as belonging to\n",
    "one of a finite number of classes $C_1, \\dots, C_n$.\n",
    "- If all the data points belong to a single class, then there is no real uncertainty, which means we’d like there to be low (or zero) entropy.\n",
    "- If the data points are evenly spread across the classes, there is a lot of uncertainty and we’d like there to be high entropy.\n",
    "\n",
    "In math terms, if $p_i$ is the proportion of data labeled as class $C_i$, we define the entropy\n",
    "as:\n",
    "\n",
    "$$ H(S) = -p_1\\log_2 p_1 - \\dots - p_n\\log_2 p_n$$\n",
    "\n",
    "with the (standard) convention that $0 \\log 0 = 0$.\n",
    "\n",
    "Without worrying too much about the grisly details, each term $−p_i \\log_2 p_i$ is nonnegative\n",
    "and is close to 0 precisely when $p_i$ is either close to 0 or close to 1.\n",
    "\n",
    "![A graph of -p log p](entropy.jpg)\n",
    "\n",
    "This means the entropy will be small when every $p_i$ is close to 0 or 1 (i.e., when most\n",
    "of the data is in a single class), and it will be larger when many of the $p_i$’s are not close\n",
    "to 0 (i.e., when the data is spread across multiple classes). This is exactly the behavior\n",
    "we desire.\n",
    "\n",
    "It is easy enough to roll all of this into a function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import math\n",
    "\n",
    "def entropy(class_probabilities: List[float]) -> float:\n",
    "    \"\"\"Given a list of class probabilities, compute the entropy\"\"\"\n",
    "    return sum(-p * math.log(p, 2)\n",
    "               for p in class_probabilities\n",
    "               if p > 0) # ignore zero probabilities since log(0) is undefined, but we will treat 0 log(0) as 0 by convention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "1.0\n",
      "0.8112781244591328\n",
      "0.8112781244591328\n",
      "0.28639695711595625\n",
      "0.8812908992306927\n"
     ]
    }
   ],
   "source": [
    "print(entropy([1.0]))\n",
    "print(entropy([0.5, 0.5]))\n",
    "print(entropy([0.25, 0.75]))\n",
    "print(entropy([0.75, 0.25]))\n",
    "print(entropy([0.95, 0.05]))\n",
    "print(entropy([0.3, 0.7]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data will consist of pairs `(input, label)`, which means that we’ll need to compute\n",
    "the class probabilities ourselves. Notice that we don’t actually care which label is\n",
    "associated with each probability, only what the probabilities are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "from collections import Counter\n",
    "\n",
    "def class_probabilities(labels: List[Any]) -> List[float]:\n",
    "    total_count = len(labels)\n",
    "    return [count / total_count\n",
    "            for count in Counter(labels).values()]\n",
    "\n",
    "def data_entropy(labels: List[Any]) -> float:\n",
    "    return entropy(class_probabilities(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "1.0\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(data_entropy(['a', 'a', 'a']))\n",
    "print(data_entropy([True, False, False, True]))\n",
    "print(data_entropy([3, 4, 4, 4]) == entropy([0.25, 0.75]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Entropy of a Partition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we’ve done so far is compute the entropy (think “uncertainty”) of a single set of\n",
    "labeled data. Now, each stage of a decision tree involves asking a question whose\n",
    "answer partitions data into one or (hopefully) more subsets.\n",
    "\n",
    "For instance, our “does it\n",
    "have more than five legs?” question partitions animals into those that have more than\n",
    "five legs (e.g., spiders) and those that don’t (e.g., echidnas).\n",
    "\n",
    "Correspondingly, we’d like some notion of the entropy that results from partitioning a\n",
    "set of data in a certain way. We want a partition to have low entropy if it splits the\n",
    "data into subsets that themselves have low entropy (i.e., are highly certain), and high\n",
    "entropy if it contains subsets that (are large and) have high entropy (i.e., are highly\n",
    "uncertain).\n",
    "\n",
    "For example, the “Australian five-cent coin” question was pretty dumb, as it partitioned the remaining animals at that point into $S_1$ = {echidna} and\n",
    "$S_2$ = {everything else}, where $S_2$ is both large and high-entropy. ($S_1$ has no entropy,\n",
    "but it represents a small fraction of the remaining “classes.”)\n",
    "\n",
    "Mathematically, if we partition our data $S$ into subsets $S_1, \\dots , S_m$ containing proportions\n",
    "$q_1, \\dots, q_m$ of the data, then we compute the entropy of the partition as a weighted\n",
    "sum:\n",
    "\n",
    "$$ H = q_1 H(S_1) + \\dots + q_m H(S_m) $$\n",
    "\n",
    "which we can implement as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition_entropy(subsets: List[List[Any]]) -> float:\n",
    "    \"\"\"Returns the entropy from this partition of data into subsets\"\"\"\n",
    "    total_count = sum(len(subset) for subset in subsets)\n",
    "\n",
    "    return sum(data_entropy(subset) * len(subset) / total_count\n",
    "               for subset in subsets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: One problem with this approach is that partitioning by an attribute\n",
    "with many different values will result in a very low entropy due to\n",
    "overfitting.\n",
    "\n",
    "For example, imagine you work for a bank and are trying\n",
    "to build a decision tree to predict which of your customers are\n",
    "likely to default on their mortgages, using some historical data as\n",
    "your training set. Imagine further that the dataset contains each\n",
    "customer’s Social Security number.\n",
    "\n",
    "Partitioning on SSN will produce\n",
    "one-person subsets, each of which necessarily has zero\n",
    "entropy. But a model that relies on SSN is *certain* not to generalize\n",
    "beyond the training set. For this reason, you should probably try to\n",
    "avoid (or bucket, if appropriate) attributes with large numbers of\n",
    "possible values when creating decision trees."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
