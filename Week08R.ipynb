{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple Regression (Continued)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "36\n",
      "[30.57901916  0.97250508 -1.86503636  0.92319974]\n",
      "x = [-100, -90, -80, -70, -60, -50, -40, -30, -20, -10, 0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
      "y = [-305, -275, -245, -215, -185, -155, -125, -95, -65, -35, -5, 25, 55, 85, 115, 145, 175, 205, 235, 265, 295]\n",
      "alpha, beta = (-5.0, 3.0)\n",
      "alpha = 22.947552413468976\n",
      "beta  = 0.9038659456058724\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rsq    = 0.3291078377836305\n",
      "rsq_sk = 0.32910783778362984\n",
      "r      = 0.57367921156656\n",
      "r^2    = 0.3291078377836299\n",
      "22.947552413468976\n",
      "0.9038659456058724\n",
      "[22.94755232  0.90386594]\n",
      "0.6800110181375751\n",
      "[99.96095009117559, 99.95263765859517, 99.91193675347006, 99.91193675347006, 100.00470211842705, 99.99027449327097, 99.96095009117559, 99.97935713972757, 99.9214102832018, 99.96512568779217, 99.99122691761812, 100.00432599511797, 100.07833662317373, 99.95263765859517, 100.00432599511797, 99.99122691761812, 99.99746144281467, 99.96095009117559, 99.99746144281467, 100.04976976287276, 99.93803411789321, 99.95263765859517, 99.99005515170228, 99.93803411789321, 100.04976976287276, 99.99005515170228, 99.89921066573656, 100.00432599511797, 100.00470211842705, 99.8688480793524, 99.97935713972757, 99.9370472765449, 99.99005515170228, 99.99005515170228, 99.99122691761812, 99.99027449327097, 99.96512568779217, 99.9370472765449, 99.99746144281467, 99.99122691761812, 99.97935713972757, 100.04976976287276, 99.96512568779217, 99.96512568779217, 99.99122691761812, 99.96512568779217, 99.93803411789321, 99.9214102832018, 99.99005515170228, 99.97935713972757, 100.04976976287276, 100.00470211842705, 99.89921066573656, 100.0855977500806, 99.99746144281467, 99.93803411789321, 100.00470211842705, 99.96512568779217, 99.91193675347006, 100.00470211842705, 99.99027449327097, 99.93803411789321, 99.90948021062609, 99.9370472765449, 100.15502248786014, 99.99122691761812, 99.96095009117559, 99.96095009117559, 99.91193675347006, 99.93803411789321, 99.92741759106502, 100.06271798669103, 99.89921066573656, 99.91193675347006, 99.99746144281467, 99.86301239848187, 99.99027449327097, 99.87106789409304, 99.9370472765449, 99.95263765859517, 99.95263765859517, 99.95263765859517, 99.92741759106502, 100.04370627041372, 99.99122691761812, 100.00470211842705, 99.97935713972757, 100.04370627041372, 99.92741759106502, 100.04370627041372, 99.99746144281467, 100.00470211842705, 99.93803411789321, 99.99122691761812, 100.00432599511797, 99.99746144281467, 99.97935713972757, 99.92741759106502, 99.97935713972757, 99.97935713972757]\n",
      "[200.24545532841594, 200.23882573978636, 200.0028348416519, 0.99078249125093, 200.179314655564, 200.20457643744942, 200.20457643744942, 0.9896539145481478, 0.7842758383216327, 0.8588263949410608, 0.99078249125093, 0.8230595720161585, 100.46468898997936, 200.179314655564, 200.20457643744942, 200.2081921050542, 0.99078249125093, 0.99078249125093, 0.99078249125093, 0.99078249125093, 200.2327152920556, 200.0028348416519, 100.46468898997936, 200.11980581455572, 0.9896539145481478, 200.179314655564, 0.7070663473305339, 200.2327152920556, 0.7122662875131835, 200.20457643744942, 200.2081921050542, 0.99078249125093, 0.8588263949410608, 0.9896539145481478, 100.46468898997936, 0.99078249125093, 0.8588263949410608, 100.46468898997936, 200.20457643744942, 200.11980581455572, 200.20457643744942, 0.8883760820676836, 200.20457643744942, 200.0028348416519, 0.7592822773191761, 200.2081921050542, 200.23882573978636, 200.2081921050542, 200.11980581455572, 0.9043976944967148, 0.8588263949410608, 200.11980581455572, 200.179314655564, 0.7070663473305339, 0.9896539145481478, 200.0028348416519, 200.23882573978636, 200.2081921050542, 200.2327152920556, 0.9896539145481478, 200.24545532841594, 0.7070663473305339, 0.6175796461520031, 200.20457643744942, 100.46468898997936, 200.31337057348375, 0.7122662875131835, 200.179314655564, 100.46468898997936, 200.11980581455572, 200.0028348416519, 0.8588263949410608, 0.99078249125093, 0.9043976944967148, 0.9896539145481478, 100.46468898997936, 0.9896539145481478, 0.8883760820676836, 0.99078249125093, 0.99078249125093, 0.8883760820676836, 200.2081921050542, 100.46468898997936, 0.9043976944967148, 200.0028348416519, 100.46468898997936, 200.2081921050542, 200.2349650782973, 0.9896539145481478, 200.339218556158, 100.46468898997936, 200.23882573978636, 0.99078249125093, 200.20457643744942, 0.8230595720161585, 0.99078249125093, 0.6921140895021592, 200.2327152920556, 0.8230595720161585, 0.7842758383216327]\n",
      "0.04794314163424293\n",
      "95.00680440654031\n"
     ]
    }
   ],
   "source": [
    "from Week07R import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard Errors of Regression Coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can take the same approach to estimating the standard errors of our regression\n",
    "coefficients. We repeatedly take a `bootstrap_sample` of our data and estimate `beta`\n",
    "based on that sample.\n",
    "\n",
    "If the coefficient corresponding to one of the independent variables\n",
    "(say, `num_friends`) doesn’t vary much across samples, then we can be confident\n",
    "that our estimate is relatively tight.\n",
    "\n",
    "If the coefficient varies greatly across samples,\n",
    "then we can’t be at all confident in our estimate.\n",
    "\n",
    "The only subtlety is that, before sampling, we’ll need to `zip` our `x` data and `y` data to\n",
    "make sure that corresponding values of the independent and dependent variables are\n",
    "sampled together. This means that bootstrap_sample will return a list of pairs `(x_i,\n",
    "y_i)`, which we’ll need to reassemble into an `x_sample` and a `y_sample`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bootstrap sample [29.99254269  1.07613367 -1.91577668  1.186479  ]\n",
      "bootstrap sample [30.48648594  0.97709026 -2.04192789  2.78209906]\n",
      "bootstrap sample [29.08889978  0.99449705 -1.66092671  1.80551976]\n",
      "bootstrap sample [30.92577745  0.96810846 -1.90614804  0.9633214 ]\n",
      "bootstrap sample [31.9090143   0.96017802 -2.00755542  1.30937309]\n",
      "bootstrap sample [31.9411427   0.91839887 -1.9199091  -1.2616056 ]\n",
      "bootstrap sample [ 3.12378020e+01  9.65547464e-01 -1.98724855e+00 -1.72733631e-02]\n",
      "bootstrap sample [29.85896791  0.98057948 -1.76696754  0.76224239]\n",
      "bootstrap sample [30.25765875  0.95966604 -1.84749705  0.89937278]\n",
      "bootstrap sample [31.03285893  0.99048218 -1.93527769  0.04375235]\n",
      "bootstrap sample [29.64814453  1.19213649 -1.97104106  0.50764887]\n",
      "bootstrap sample [31.26240567  0.9619901  -1.92961632  0.9808242 ]\n",
      "bootstrap sample [29.7519696   0.98349785 -1.89373714  2.52523667]\n",
      "bootstrap sample [31.05247365  0.964177   -2.04809218  0.57252437]\n",
      "bootstrap sample [31.36499487  0.94962039 -1.95565568 -0.36168892]\n",
      "bootstrap sample [32.1391261   0.93135134 -1.86115349  0.31444917]\n",
      "bootstrap sample [29.07869928  1.06942495 -1.74106906  1.74660131]\n",
      "bootstrap sample [30.47853718  1.0306084  -1.87546543  0.93240545]\n",
      "bootstrap sample [29.118747    1.0268878  -1.66287285  0.77200734]\n",
      "bootstrap sample [29.91810592  0.94771359 -1.72524314  0.69016003]\n",
      "bootstrap sample [30.9725508   0.98221304 -2.03976368  0.91637637]\n",
      "bootstrap sample [31.67028362  0.98564891 -1.95731717  1.11833275]\n",
      "bootstrap sample [30.66350243  1.00220195 -1.69190739  0.15102007]\n",
      "bootstrap sample [29.53247582  0.98120901 -1.77409101  1.29093426]\n",
      "bootstrap sample [31.36821503  0.96440808 -1.85965728  0.14273678]\n",
      "bootstrap sample [30.50072882  0.91520496 -1.72101127  1.13846041]\n",
      "bootstrap sample [29.00197244  1.01386647 -1.65714744  1.93571682]\n",
      "bootstrap sample [28.70952348  1.2152913  -1.93988165  2.18774854]\n",
      "bootstrap sample [32.27637122  0.88141644 -1.77111382 -0.26732895]\n",
      "bootstrap sample [30.33481904  0.96994352 -1.8595447   1.18287721]\n",
      "bootstrap sample [31.31365769  0.87563818 -1.80716714  0.64513445]\n",
      "bootstrap sample [30.82621294  1.09509729 -2.0656235   1.12982912]\n",
      "bootstrap sample [29.6298574   1.05072285 -1.86487982  1.74546358]\n",
      "bootstrap sample [30.24729906  1.02397437 -1.7402935   0.72949528]\n",
      "bootstrap sample [30.81220624  0.91671031 -1.8554843   0.3276007 ]\n",
      "bootstrap sample [30.6476822   1.02416117 -1.92964505  0.86470864]\n",
      "bootstrap sample [30.27775727  0.96165225 -1.92093166  1.36598217]\n",
      "bootstrap sample [30.94874614  0.91327775 -1.78634616  0.52424744]\n",
      "bootstrap sample [29.40266401  1.03464604 -1.79428613  1.70154525]\n",
      "bootstrap sample [30.41837804  0.95678627 -1.6665491   0.14670904]\n",
      "bootstrap sample [29.80954005  1.03271683 -1.88634234  2.2513621 ]\n",
      "bootstrap sample [30.59751792  0.96801401 -1.91841684  0.27389774]\n",
      "bootstrap sample [32.04435679  0.85079536 -1.7924627  -0.29738921]\n",
      "bootstrap sample [30.54549214  0.97841181 -1.92541186  1.43901177]\n",
      "bootstrap sample [30.71307553  1.01641181 -2.03984644  0.55130752]\n",
      "bootstrap sample [28.66709948  1.19389698 -1.9009651   3.1420133 ]\n",
      "bootstrap sample [31.03873982  0.93613473 -1.75847565 -0.52651966]\n",
      "bootstrap sample [31.27639953  0.98229338 -1.96127708  0.26805169]\n",
      "bootstrap sample [30.6059597   0.92532554 -1.67836968  0.22166594]\n",
      "bootstrap sample [29.92359568  1.01285075 -1.84132103  1.29117114]\n",
      "bootstrap sample [29.30797044  1.03734968 -1.89568883  1.92659126]\n",
      "bootstrap sample [31.91319422  1.04609031 -2.19160616  0.48603119]\n",
      "bootstrap sample [30.71393975  0.9788512  -1.87738266 -0.36637238]\n",
      "bootstrap sample [30.13585832  0.98351202 -1.98938429  1.99437482]\n",
      "bootstrap sample [30.42083978  0.9031125  -1.68217428  0.2373177 ]\n",
      "bootstrap sample [30.99408345  0.95465657 -1.90338029  0.92479059]\n",
      "bootstrap sample [31.32268654  0.9352907  -1.95931755  1.83730899]\n",
      "bootstrap sample [29.46781258  1.0505605  -1.80252331  2.53989638]\n",
      "bootstrap sample [30.76188207  0.94102822 -1.77014108  0.60320139]\n",
      "bootstrap sample [30.49096529  0.98463056 -1.85414689  1.58829403]\n",
      "bootstrap sample [31.2502478   0.99758743 -2.00835181  0.73105601]\n",
      "bootstrap sample [31.02161793  0.93700546 -1.85127036  1.1160914 ]\n",
      "bootstrap sample [30.25342375  0.99500122 -1.92243456  1.3425932 ]\n",
      "bootstrap sample [30.5595717   0.99123793 -1.86679492  1.43675266]\n",
      "bootstrap sample [31.63312479  0.94989622 -1.98741229  0.30998259]\n",
      "bootstrap sample [31.56211238  0.98464978 -2.0202036  -1.61930462]\n",
      "bootstrap sample [30.22194368  0.95710547 -1.78563548 -0.06531763]\n",
      "bootstrap sample [31.54053723  0.90216552 -1.92874226  0.17090924]\n",
      "bootstrap sample [29.89955854  0.97341514 -1.7599185   1.07495819]\n",
      "bootstrap sample [29.82046596  1.10159911 -2.00768789  2.18607594]\n",
      "bootstrap sample [29.61596075  0.95988021 -1.719295    0.88667752]\n",
      "bootstrap sample [30.92661629  0.88875378 -1.76717895  0.9920636 ]\n",
      "bootstrap sample [30.64094137  0.93878884 -1.89342781  1.21097001]\n",
      "bootstrap sample [32.28397193  0.91062516 -1.89782808 -0.03617572]\n",
      "bootstrap sample [30.99317612  0.9661792  -1.83173208  1.09954545]\n",
      "bootstrap sample [30.56416664  0.95988531 -1.8139402   0.98820395]\n",
      "bootstrap sample [ 3.14153537e+01  9.00532945e-01 -1.81103790e+00 -3.03571341e-02]\n",
      "bootstrap sample [31.14665667  0.97473733 -1.95857281  0.68116921]\n",
      "bootstrap sample [30.08350189  1.01621882 -1.85567253  2.12044309]\n",
      "bootstrap sample [30.92324777  0.95122768 -1.88033218  0.59690737]\n",
      "bootstrap sample [29.88691072  0.9796443  -1.74556106  0.68790523]\n",
      "bootstrap sample [31.01071273  0.9729354  -1.92594753  1.39052056]\n",
      "bootstrap sample [29.60638202  0.97641087 -1.80336459  1.05155955]\n",
      "bootstrap sample [31.2382382   1.039534   -2.10527429  0.44844466]\n",
      "bootstrap sample [29.995068    0.99617282 -1.97248542  2.74681519]\n",
      "bootstrap sample [29.80998865  1.04410452 -1.88003206  0.57058818]\n",
      "bootstrap sample [31.7270947   0.9030095  -1.92755151  0.21619579]\n",
      "bootstrap sample [30.27284299  1.04703429 -2.00015506  2.47389462]\n",
      "bootstrap sample [30.80356591  0.97209252 -1.91861069  1.43934033]\n",
      "bootstrap sample [30.54668445  1.00930119 -1.88267446  0.70052527]\n",
      "bootstrap sample [29.08396111  1.01448633 -1.81614427  2.23458827]\n",
      "bootstrap sample [ 3.08513918e+01  9.08591142e-01 -1.84462594e+00 -2.13534798e-02]\n",
      "bootstrap sample [29.37422057  1.15458579 -1.9185513   2.56373371]\n",
      "bootstrap sample [30.88605112  0.98556532 -1.95599415  0.55921645]\n",
      "bootstrap sample [28.95006684  1.02646793 -1.79966023  2.07098061]\n",
      "bootstrap sample [31.43574111  0.9212371  -1.76126186  0.19851575]\n",
      "bootstrap sample [29.42564282  1.1401611  -1.99657433  2.20523331]\n",
      "bootstrap sample [31.58597817  0.91031616 -1.85908076 -0.41749676]\n",
      "bootstrap sample [30.55160782  0.94217826 -1.90223172  1.14516614]\n",
      "bootstrap sample [31.15968733  0.98288657 -2.01685779  1.34821517]\n"
     ]
    }
   ],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "def estimate_sample_beta(pairs: List[Tuple[Vector, float]]):\n",
    "    x_sample = [x for x, _ in pairs]\n",
    "    y_sample = [y for _, y in pairs]\n",
    "    beta = least_squares_fit(x_sample, y_sample)\n",
    "    print(\"bootstrap sample\", beta)\n",
    "    return beta\n",
    "\n",
    "random.seed(0) # so that you get the same results when you run this code\n",
    "\n",
    "# This might take a little while\n",
    "bootstrap_betas = bootstrap_statistic(list(zip(inputs, daily_minutes_good)),\n",
    "                                      estimate_sample_beta,\n",
    "                                      100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After which we can estimate the standard deviation of each coefficient:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[30.55408300846275, 0.9850730066827625, -1.8758456514309334, 0.9532287844812876]\n",
      "[0.8464395054173437, 0.06587067787018257, 0.10759600504197035, 0.8852940607411107]\n"
     ]
    }
   ],
   "source": [
    "from statistics import mean\n",
    "\n",
    "bootstrap_standard_errors = [stdev([beta[i] for beta in bootstrap_betas])\n",
    "                             for i in range(4)]\n",
    "\n",
    "bootstrap_param_means = [mean([beta[i] for beta in bootstrap_betas])\n",
    "                             for i in range(4)]\n",
    "\n",
    "print(bootstrap_param_means)\n",
    "print(bootstrap_standard_errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# [0.846, # constant term, actual error = 1.19\n",
    "#  0.066, # num_friends,   actual error = 0.080\n",
    "#  0.107, # work_hours,    actual error = 0.127\n",
    "#  0.885] # phd,           actual error = 0.998\n",
    "```\n",
    "\n",
    "(We would likely get better estimates if we collected a lot more samples to estimate each `beta`, but we don’t have all day.)\n",
    "\n",
    "If you have some background in statistics, you could also go ahead and try to perform a hypothesis test to see if each $\\beta_i$ parameter equals to 0 or not. For which you may be able to compute a \"p-value\", which is the probablility value that has the following interpretation: if we assume that the true $\\beta_i$ parameter is essentially 0, what is the probability of obtaining an actual $\\beta_i$ this extreme from the sample?\n",
    "\n",
    "Or put it simply, **if the p-value for a parameter lower than some signficance threshold (e.g., 0.05 is a common one), then we assume that parameter is actually significant** in predicting the response variable. Please keep this in mind when you use some regression software and it outputs the p-values for the parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, you’d often like to apply linear regression to datasets with large numbers\n",
    "of variables. This creates a couple of extra wrinkles.\n",
    "- First, the more variables you use, the more likely you are to overfit your model to the training set.\n",
    "- Second, the more nonzero coefficients you have, the harder it is to make sense of them (If the goal is to explain some phenomenon, a sparse model with three factors might be more useful than a slightly better model with hundreds).\n",
    "\n",
    "*Regularization* is an approach in which we add to the error term a penalty that gets\n",
    "larger as beta gets larger. We then minimize the combined error and penalty. The\n",
    "more importance we place on the penalty term, the more we discourage large coefficients.\n",
    "\n",
    "For example, in *ridge regression*, we add a penalty proportional to the sum of the\n",
    "squares of the `beta_i` (except that typically we don’t penalize `beta_0`, the constant\n",
    "term):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alpha is a *hyperparameter* controlling how harsh the penalty is\n",
    "# sometimes it's called \"lambda\" but that already means something in Python\n",
    "def ridge_penalty(beta: Vector, alpha: float) -> float:\n",
    "    return alpha * dot(beta[1:], beta[1:])\n",
    "\n",
    "def squared_error_ridge(x: Vector,\n",
    "                        y: float,\n",
    "                        beta: Vector,\n",
    "                        alpha: float) -> float:\n",
    "    \"\"\"estimate error plus ridge penalty on beta\"\"\"\n",
    "    return error(x, y, beta) ** 2 + ridge_penalty(beta, alpha)\n",
    "\n",
    "def sum_of_sqerrors_ridge(xs: List[Vector], ys: Vector, beta: Vector, alpha) -> float:\n",
    "    return sum(squared_error_ridge(x_i, y_i, beta, alpha) for x_i, y_i in zip(xs, ys))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then plug this into an optimizer in the usual way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_squares_fit_ridge(xs: List[Vector], ys: Vector, alpha: float) -> Vector:\n",
    "    \"\"\"\n",
    "    Find the beta that minimizes the sum of squared errors\n",
    "    assuming the model y = dot(x, beta).\n",
    "    \"\"\"\n",
    "    def object_fun (beta: Vector):\n",
    "        return sum_of_sqerrors_ridge (xs, ys, beta, alpha)\n",
    "    \n",
    "    # Start with a random guess\n",
    "    guess = [random.random() for _ in xs[0]]\n",
    "\n",
    "    optim = minimize (object_fun, guess, method='BFGS')\n",
    "    \n",
    "    return optim.x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With `alpha` set to 0, there’s no penalty at all and we get the same results as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta = [30.57901795  0.97250514 -1.86503625  0.92319992]\n",
      "sum of squared betas = 5.276424566658393\n",
      "R^2 = 0.6800110181375747\n"
     ]
    }
   ],
   "source": [
    "random.seed(0)\n",
    "beta_0 = least_squares_fit_ridge(inputs, daily_minutes_good, 0.0)\n",
    "\n",
    "print(f\"beta = {beta_0}\")\n",
    "print(f\"sum of squared betas = {dot(beta_0[1:], beta_0[1:])}\")\n",
    "print(f\"R^2 = {multiple_r_squared(inputs, daily_minutes_good, beta_0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we increase `alpha`, the goodness of fit gets worse, but the size of `beta` gets smaller:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta = [30.83476977  0.95146517 -1.84689046  0.54706714]\n",
      "sum of squared betas = 4.615572784013092\n",
      "R^2 = 0.6797434135107308\n"
     ]
    }
   ],
   "source": [
    "beta_0_1 = least_squares_fit_ridge(inputs, daily_minutes_good, 0.1)\n",
    "\n",
    "print(f\"beta = {beta_0_1}\")\n",
    "print(f\"sum of squared betas = {dot(beta_0_1[1:], beta_0_1[1:])}\")\n",
    "print(f\"R^2 = {multiple_r_squared(inputs, daily_minutes_good, beta_0_1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta = [30.66730883  0.90642357 -1.69184946  0.09053743]\n",
      "sum of squared betas = 3.6921553253518407\n",
      "R^2 = 0.6755909879053199\n"
     ]
    }
   ],
   "source": [
    "beta_1 = least_squares_fit_ridge(inputs, daily_minutes_good, 1)\n",
    "\n",
    "print(f\"beta = {beta_1}\")\n",
    "print(f\"sum of squared betas = {dot(beta_1[1:], beta_1[1:])}\")\n",
    "print(f\"R^2 = {multiple_r_squared(inputs, daily_minutes_good, beta_1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta = [ 2.83782374e+01  7.28016018e-01 -9.15431147e-01 -1.83800348e-02]\n",
      "sum of squared betas = 1.3683593325404881\n",
      "R^2 = 0.5737151109381915\n"
     ]
    }
   ],
   "source": [
    "beta_10 = least_squares_fit_ridge(inputs, daily_minutes_good, 10)\n",
    "\n",
    "print(f\"beta = {beta_10}\")\n",
    "print(f\"sum of squared betas = {dot(beta_10[1:], beta_10[1:])}\")\n",
    "print(f\"R^2 = {multiple_r_squared(inputs, daily_minutes_good, beta_10)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In particular, the coefficient on “PhD” vanishes as we increase the penalty, which\n",
    "accords with our previous result that it wasn’t significantly different from 0.\n",
    "\n",
    "Note: usually you’d want to rescale your data before using this\n",
    "approach. After all, if you changed years of experience to centuries\n",
    "of experience, its least squares coefficient would increase by a factor\n",
    "of 100 and suddenly get penalized much more, even though it’s\n",
    "the same model.\n",
    "\n",
    "Another approach is *lasso regression*, which uses the penalty:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lasso_penalty(beta, alpha):\n",
    "    return alpha * sum(abs(beta_i) for beta_i in beta[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whereas the ridge penalty shrank the coefficients overall, the lasso penalty tends to\n",
    "force coefficients to be 0, which makes it good for learning sparse models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression in scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Documentation Link: https://scikit-learn.org/stable/modules/linear_model.html\n",
    "\n",
    "This example uses the only the first feature of the `diabetes` dataset, in order to illustrate a two-dimensional plot of this regression technique. The straight line can be seen in the plot, showing how linear regression attempts to draw a straight line that will best minimize the residual sum of squares between the observed responses in the dataset, and the responses predicted by the linear approximation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept: \n",
      " 152.91886182616167\n",
      "Coefficients: \n",
      " [938.23786125]\n",
      "Mean squared error: 2548.07\n",
      "R-Squared: 0.47\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# https://scikit-learn.org/stable/auto_examples/linear_model/plot_ols.html#sphx-glr-auto-examples-linear-model-plot-ols-py\n",
    "# Code source: Jaques Grobler\n",
    "# License: BSD 3 clause\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Load the diabetes dataset\n",
    "diabetes_X, diabetes_y = datasets.load_diabetes(return_X_y=True)\n",
    "\n",
    "# Use only one feature at column 2, for illustrative purposes only so that it's easy to plot the graph later\n",
    "# Note that in scikit-learn, there's no inherent distinction between Simple Linear Regression and Multiple Linear Regression\n",
    "# Just pass in the desired X, which contains your features (1 feature: simple linear regression. n features: multiple regression)\n",
    "# There's no need to include a column of 1's in your X here, it can be controlled by a parameter named \"fit_intercept\",\n",
    "# when creating the LinearRegression object later.\n",
    "diabetes_X = diabetes_X[:, 2:3]\n",
    "\n",
    "# Split the data into training/testing sets\n",
    "diabetes_X_train = diabetes_X[:-20]\n",
    "diabetes_X_test = diabetes_X[-20:]\n",
    "\n",
    "# Split the targets into training/testing sets\n",
    "diabetes_y_train = diabetes_y[:-20]\n",
    "diabetes_y_test = diabetes_y[-20:]\n",
    "\n",
    "# Create linear regression object\n",
    "regr = linear_model.LinearRegression()\n",
    "\n",
    "# Train the model using the training sets\n",
    "regr.fit(diabetes_X_train, diabetes_y_train)\n",
    "\n",
    "# Make predictions using the testing set\n",
    "diabetes_y_pred = regr.predict(diabetes_X_test)\n",
    "\n",
    "# The intercept:\n",
    "print('Intercept: \\n', regr.intercept_)\n",
    "# The coefficients\n",
    "print('Coefficients: \\n', regr.coef_)\n",
    "# The mean squared error\n",
    "print('Mean squared error: %.2f'\n",
    "      % mean_squared_error(diabetes_y_test, diabetes_y_pred))\n",
    "# The coefficient of determination: 1 is perfect prediction\n",
    "print('R-Squared: %.2f'\n",
    "      % r2_score(diabetes_y_test, diabetes_y_pred))\n",
    "\n",
    "# Plot outputs\n",
    "plt.scatter(diabetes_X_test, diabetes_y_test,  color='black')\n",
    "plt.plot(diabetes_X_test, diabetes_y_pred, color='blue', linewidth=3)\n",
    "\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-Squared: 0.96\n"
     ]
    }
   ],
   "source": [
    "y_true = [1, 1.5, 3, 2.7]\n",
    "y_pred = [0.9, 1.4, 2.9, 3]\n",
    "print('R-Squared: %.2f'\n",
    "      % r2_score(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here's a super simple example to fit ridge regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07275541795665635\n",
      "[0.49226006 0.14551084]\n"
     ]
    }
   ],
   "source": [
    "reg = linear_model.Ridge(alpha=.5)\n",
    "reg.fit([[0, 0], [0.2, 1], [1, 1]], [0, .1, 1])\n",
    "print(reg.intercept_)\n",
    "print(reg.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here's lasso regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1666666666666667\n",
      "[0.5 0. ]\n"
     ]
    }
   ],
   "source": [
    "reg = linear_model.Lasso(alpha=0.1)\n",
    "reg.fit([[0, 0], [0.2, 1], [1, 1]], [0, .1, 1])\n",
    "print(reg.intercept_)\n",
    "print(reg.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which can be compared with simple linear regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.220446049250313e-16\n",
      "[ 1.125 -0.125]\n"
     ]
    }
   ],
   "source": [
    "reg = linear_model.LinearRegression()\n",
    "reg.fit([[0, 0], [0.2, 1], [1, 1]], [0, .1, 1])\n",
    "print(reg.intercept_)\n",
    "print(reg.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also do polynomial regressions (e.g., quadratic regression, cubic regression, with or without any interaction terms) by expanding the design matrix x with new columns representing the polynomial powers or products of the original columns and then pass the new X to a linear regression object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  1]\n",
      " [ 2  3]\n",
      " [ 4  5]\n",
      " [ 6  7]\n",
      " [ 8  9]\n",
      " [10 11]\n",
      " [12 13]\n",
      " [14 15]\n",
      " [16 17]\n",
      " [18 19]]\n",
      "[[  0.   1.   0.   0.   1.]\n",
      " [  2.   3.   4.   6.   9.]\n",
      " [  4.   5.  16.  20.  25.]\n",
      " [  6.   7.  36.  42.  49.]\n",
      " [  8.   9.  64.  72.  81.]\n",
      " [ 10.  11. 100. 110. 121.]\n",
      " [ 12.  13. 144. 156. 169.]\n",
      " [ 14.  15. 196. 210. 225.]\n",
      " [ 16.  17. 256. 272. 289.]\n",
      " [ 18.  19. 324. 342. 361.]]\n",
      "intercept = -239.81975637033054\n",
      "coefficients = [38.34184337 38.34184337]\n",
      "Linear Regression R-Squared = 0.9126048017061228\n",
      "intercept = 16.969375715729825\n",
      "coefficients = [-2.16578462 -2.16578462  3.66309735  1.49731272 -0.6684719 ]\n",
      "Quadratic Regression R-Squared = 0.9927697073095132\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "X = np.arange(20).reshape(10, 2)\n",
    "print(X)\n",
    "\n",
    "y = [(x_i[0] + x_i[1] + np.random.randn()) ** 2  for x_i in X]\n",
    "\n",
    "poly = PolynomialFeatures(2, include_bias = False) # the initial columns of 1's not needed for LinearRegression()\n",
    "X2 = poly.fit_transform(X)\n",
    "\n",
    "print(X2)\n",
    "\n",
    "reg = linear_model.LinearRegression()\n",
    "reg.fit(X, y)\n",
    "print(f\"intercept = {reg.intercept_}\")\n",
    "print(f\"coefficients = {reg.coef_}\")\n",
    "print(f\"Linear Regression R-Squared = {r2_score(y, reg.predict(X))}\")\n",
    "\n",
    "reg.fit(X2, y)\n",
    "print(f\"intercept = {reg.intercept_}\")\n",
    "print(f\"coefficients = {reg.coef_}\")\n",
    "print(f\"Quadratic Regression R-Squared = {r2_score(y, reg.predict(X2))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression in statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0.\n",
      " 1. 1. 1. 0. 1. 1. 0. 1.]\n",
      "[[ 1.    2.66 20.    0.  ]\n",
      " [ 1.    2.89 22.    0.  ]\n",
      " [ 1.    3.28 24.    0.  ]\n",
      " [ 1.    2.92 12.    0.  ]\n",
      " [ 1.    4.   21.    0.  ]\n",
      " [ 1.    2.86 17.    0.  ]\n",
      " [ 1.    2.76 17.    0.  ]\n",
      " [ 1.    2.87 21.    0.  ]\n",
      " [ 1.    3.03 25.    0.  ]\n",
      " [ 1.    3.92 29.    0.  ]\n",
      " [ 1.    2.63 20.    0.  ]\n",
      " [ 1.    3.32 23.    0.  ]\n",
      " [ 1.    3.57 23.    0.  ]\n",
      " [ 1.    3.26 25.    0.  ]\n",
      " [ 1.    3.53 26.    0.  ]\n",
      " [ 1.    2.74 19.    0.  ]\n",
      " [ 1.    2.75 25.    0.  ]\n",
      " [ 1.    2.83 19.    0.  ]\n",
      " [ 1.    3.12 23.    1.  ]\n",
      " [ 1.    3.16 25.    1.  ]\n",
      " [ 1.    2.06 22.    1.  ]\n",
      " [ 1.    3.62 28.    1.  ]\n",
      " [ 1.    2.89 14.    1.  ]\n",
      " [ 1.    3.51 26.    1.  ]\n",
      " [ 1.    3.54 24.    1.  ]\n",
      " [ 1.    2.83 27.    1.  ]\n",
      " [ 1.    3.39 17.    1.  ]\n",
      " [ 1.    2.67 24.    1.  ]\n",
      " [ 1.    3.65 21.    1.  ]\n",
      " [ 1.    4.   23.    1.  ]\n",
      " [ 1.    3.1  21.    1.  ]\n",
      " [ 1.    2.39 19.    1.  ]]\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.416\n",
      "Model:                            OLS   Adj. R-squared:                  0.353\n",
      "Method:                 Least Squares   F-statistic:                     6.646\n",
      "Date:                Mon, 02 Nov 2020   Prob (F-statistic):            0.00157\n",
      "Time:                        19:00:16   Log-Likelihood:                -12.978\n",
      "No. Observations:                  32   AIC:                             33.96\n",
      "Df Residuals:                      28   BIC:                             39.82\n",
      "Df Model:                           3                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -1.4980      0.524     -2.859      0.008      -2.571      -0.425\n",
      "x1             0.4639      0.162      2.864      0.008       0.132       0.796\n",
      "x2             0.0105      0.019      0.539      0.594      -0.029       0.050\n",
      "x3             0.3786      0.139      2.720      0.011       0.093       0.664\n",
      "==============================================================================\n",
      "Omnibus:                        0.176   Durbin-Watson:                   2.346\n",
      "Prob(Omnibus):                  0.916   Jarque-Bera (JB):                0.167\n",
      "Skew:                           0.141   Prob(JB):                        0.920\n",
      "Kurtosis:                       2.786   Cond. No.                         176.\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "spector_data = sm.datasets.spector.load()\n",
    "\n",
    "# unlike scikit-learn, you are required to add a column of 1's if you want to fit the intercept\n",
    "spector_data.exog1 = sm.add_constant(spector_data.exog) \n",
    "\n",
    "print(spector_data.endog) # endogeneous/response/dependent/y variable\n",
    "print(spector_data.exog1)  # exogenous/explanatory/independent/x variable\n",
    "\n",
    "# Fit and summarize Ordinary Least Squares (OLS) model\n",
    "# Be sure to pass in y first, followed by X!\n",
    "mod = sm.OLS(spector_data.endog, spector_data.exog1)\n",
    "res = mod.fit()\n",
    "print(res.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  1.       2.66    20.       0.       7.0756  53.2      0.     400.\n",
      "    0.       0.    ]\n",
      " [  1.       2.89    22.       0.       8.3521  63.58     0.     484.\n",
      "    0.       0.    ]\n",
      " [  1.       3.28    24.       0.      10.7584  78.72     0.     576.\n",
      "    0.       0.    ]\n",
      " [  1.       2.92    12.       0.       8.5264  35.04     0.     144.\n",
      "    0.       0.    ]\n",
      " [  1.       4.      21.       0.      16.      84.       0.     441.\n",
      "    0.       0.    ]\n",
      " [  1.       2.86    17.       0.       8.1796  48.62     0.     289.\n",
      "    0.       0.    ]\n",
      " [  1.       2.76    17.       0.       7.6176  46.92     0.     289.\n",
      "    0.       0.    ]\n",
      " [  1.       2.87    21.       0.       8.2369  60.27     0.     441.\n",
      "    0.       0.    ]\n",
      " [  1.       3.03    25.       0.       9.1809  75.75     0.     625.\n",
      "    0.       0.    ]\n",
      " [  1.       3.92    29.       0.      15.3664 113.68     0.     841.\n",
      "    0.       0.    ]\n",
      " [  1.       2.63    20.       0.       6.9169  52.6      0.     400.\n",
      "    0.       0.    ]\n",
      " [  1.       3.32    23.       0.      11.0224  76.36     0.     529.\n",
      "    0.       0.    ]\n",
      " [  1.       3.57    23.       0.      12.7449  82.11     0.     529.\n",
      "    0.       0.    ]\n",
      " [  1.       3.26    25.       0.      10.6276  81.5      0.     625.\n",
      "    0.       0.    ]\n",
      " [  1.       3.53    26.       0.      12.4609  91.78     0.     676.\n",
      "    0.       0.    ]\n",
      " [  1.       2.74    19.       0.       7.5076  52.06     0.     361.\n",
      "    0.       0.    ]\n",
      " [  1.       2.75    25.       0.       7.5625  68.75     0.     625.\n",
      "    0.       0.    ]\n",
      " [  1.       2.83    19.       0.       8.0089  53.77     0.     361.\n",
      "    0.       0.    ]\n",
      " [  1.       3.12    23.       1.       9.7344  71.76     3.12   529.\n",
      "   23.       1.    ]\n",
      " [  1.       3.16    25.       1.       9.9856  79.       3.16   625.\n",
      "   25.       1.    ]\n",
      " [  1.       2.06    22.       1.       4.2436  45.32     2.06   484.\n",
      "   22.       1.    ]\n",
      " [  1.       3.62    28.       1.      13.1044 101.36     3.62   784.\n",
      "   28.       1.    ]\n",
      " [  1.       2.89    14.       1.       8.3521  40.46     2.89   196.\n",
      "   14.       1.    ]\n",
      " [  1.       3.51    26.       1.      12.3201  91.26     3.51   676.\n",
      "   26.       1.    ]\n",
      " [  1.       3.54    24.       1.      12.5316  84.96     3.54   576.\n",
      "   24.       1.    ]\n",
      " [  1.       2.83    27.       1.       8.0089  76.41     2.83   729.\n",
      "   27.       1.    ]\n",
      " [  1.       3.39    17.       1.      11.4921  57.63     3.39   289.\n",
      "   17.       1.    ]\n",
      " [  1.       2.67    24.       1.       7.1289  64.08     2.67   576.\n",
      "   24.       1.    ]\n",
      " [  1.       3.65    21.       1.      13.3225  76.65     3.65   441.\n",
      "   21.       1.    ]\n",
      " [  1.       4.      23.       1.      16.      92.       4.     529.\n",
      "   23.       1.    ]\n",
      " [  1.       3.1     21.       1.       9.61    65.1      3.1    441.\n",
      "   21.       1.    ]\n",
      " [  1.       2.39    19.       1.       5.7121  45.41     2.39   361.\n",
      "   19.       1.    ]]\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.471\n",
      "Model:                            OLS   Adj. R-squared:                  0.286\n",
      "Method:                 Least Squares   F-statistic:                     2.556\n",
      "Date:                Mon, 02 Nov 2020   Prob (F-statistic):             0.0373\n",
      "Time:                        19:00:16   Log-Likelihood:                -11.404\n",
      "No. Observations:                  32   AIC:                             40.81\n",
      "Df Residuals:                      23   BIC:                             54.00\n",
      "Df Model:                           8                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const          2.6072      4.700      0.555      0.584      -7.115      12.329\n",
      "x1            -1.8482      2.251     -0.821      0.420      -6.504       2.808\n",
      "x2            -0.0427      0.185     -0.231      0.820      -0.426       0.341\n",
      "x3             0.0797      0.625      0.128      0.900      -1.212       1.372\n",
      "x4             0.4713      0.343      1.372      0.183      -0.239       1.182\n",
      "x5            -0.0324      0.070     -0.462      0.649      -0.178       0.113\n",
      "x6             0.0896      0.414      0.216      0.831      -0.768       0.947\n",
      "x7             0.0037      0.005      0.749      0.462      -0.007       0.014\n",
      "x8            -0.0054      0.044     -0.124      0.902      -0.096       0.085\n",
      "x9             0.0797      0.625      0.128      0.900      -1.212       1.372\n",
      "==============================================================================\n",
      "Omnibus:                        1.220   Durbin-Watson:                   2.482\n",
      "Prob(Omnibus):                  0.543   Jarque-Bera (JB):                0.836\n",
      "Skew:                           0.394   Prob(JB):                        0.659\n",
      "Kurtosis:                       2.918   Cond. No.                     3.42e+18\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The smallest eigenvalue is 7.62e-31. This might indicate that there are\n",
      "strong multicollinearity problems or that the design matrix is singular.\n"
     ]
    }
   ],
   "source": [
    "# Quadratic Regression\n",
    "# We will just borrow PolynomialFeatures from scikit-learn to generate the appropriate columns for polynomial regression\n",
    "poly = PolynomialFeatures(2, include_bias = True)\n",
    "spector_data.exog2 = poly.fit_transform (spector_data.exog)\n",
    "\n",
    "print(spector_data.exog2)\n",
    "\n",
    "mod2 = sm.OLS(spector_data.endog, spector_data.exog2)\n",
    "res2 = mod2.fit()\n",
    "print(res2.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.35026798  0.2020077  -0.00237391  0.30608729]\n"
     ]
    }
   ],
   "source": [
    "# Ridge Regression, call fit_regularized instead of fit, set L1_wt = 0\n",
    "res_ridge = mod.fit_regularized(alpha = 0.05, L1_wt = 0)\n",
    "print(res_ridge.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.         0.04180365 0.00709658 0.16680278]\n"
     ]
    }
   ],
   "source": [
    "# Lasso Regression, call fit_regularized instead of fit, set L1_wt = 1\n",
    "res_lasso = mod.fit_regularized(alpha = 0.05, L1_wt = 1) # Lasso\n",
    "print(res_lasso.params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
