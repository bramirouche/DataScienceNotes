{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the VP is pretty impressed with your predictive model, she thinks you can\n",
    "do better. To that end, you’ve collected additional data: you know how many hours\n",
    "each of your users works each day, and whether they have a PhD. You’d like to use this\n",
    "additional data to improve your model.\n",
    "\n",
    "Accordingly, you hypothesize a linear model with more independent variables:\n",
    "\n",
    "$$ \\large \\textrm{minutes} = \\alpha + \\beta_1 \\textrm{friends} + \\beta_2 \\textrm{work hours} + \\beta_3 \\textrm{phd} + \\epsilon $$\n",
    "\n",
    "Obviously, whether a user has a PhD is not a number—but, as we mentioned earlier discussions, we can introduce a dummy variable that equals 1 for users with PhDs and\n",
    "0 for users without, after which it’s just as numeric as the other variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that when discussing Simple Linear Regression we fit a model of the form:\n",
    "\n",
    "$$ \\large y_i = \\alpha + \\beta x_i + \\epsilon_i $$\n",
    "\n",
    "Now imagine that each input $x_i$ is not a single number but rather a vector of $k$ numbers,\n",
    "$x_{i1}, \\dots, x_{ik}$. The multiple (linear) regression model assumes that:\n",
    "\n",
    "$$ \\large y_i = \\alpha + \\beta_1 x_{i1} + \\dots + \\beta_k x_{ik} + \\epsilon_i $$\n",
    "\n",
    "In multiple regression the vector of parameters is usually called $β$. We’ll want this to\n",
    "include the constant term as well, which we can achieve by adding a column of 1s to\n",
    "our data:\n",
    "\n",
    "```python\n",
    "beta = [alpha, beta_1, ..., beta_k]\n",
    "```\n",
    "and:\n",
    "```python\n",
    "x_i = [1, x_i1, ..., x_ik]\n",
    "```\n",
    "Then our model is just:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "Vector = List[float]\n",
    "\n",
    "def dot(v: Vector, w: Vector) -> float:\n",
    "    \"\"\"Computes v_1 * w_1 + ... + v_n * w_n\"\"\"\n",
    "    assert len(v) == len(w), \"vectors must be same length\"\n",
    "\n",
    "    return sum(v_i * w_i for v_i, w_i in zip(v, w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x: Vector, beta: Vector) -> float:\n",
    "    \"\"\"assumes that the first element of x is 1\"\"\"\n",
    "    return dot(x, beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this particular case, our independent variable x will be a list of vectors, each of\n",
    "which looks like this:\n",
    "```python\n",
    "[1,    # constant term\n",
    " 49,   # number of friends\n",
    " 4,    # work hours per day\n",
    " 0]    # doesn't have PhD\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Assumptions of the Least Squares Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a couple of further assumptions that are required for this model (and our\n",
    "solution) to make sense.\n",
    "\n",
    "The first is that the columns of x are *linearly independent*—that there’s no way to write\n",
    "any one as a weighted sum of some of the others (or perfectly predict one column based on the other columns). If this assumption fails, it’s impossible\n",
    "to estimate `beta`. To see this in an extreme case, imagine we had an extra field\n",
    "`num_acquaintances` in our data that for every user was exactly equal to `num_friends`.\n",
    "\n",
    "Then, starting with any `beta`, if we add any amount to the `num_friends` coefficient\n",
    "and subtract that same amount from the `num_acquaintances` coefficient, the model’s\n",
    "predictions will remain unchanged. This means that there’s no way to find the coefficient\n",
    "for `num_friends`. (Usually violations of this assumption won’t be so obvious.)\n",
    "\n",
    "The second important assumption is that the columns of `x` are all uncorrelated with\n",
    "the errors $\\epsilon$. If this fails to be the case, our estimates of `beta` will be systematically\n",
    "wrong.\n",
    "\n",
    "For instance, earlier we built a simple linear regression model that predicted that each additional\n",
    "friend was associated with an extra 0.90 daily minutes on the site.\n",
    "\n",
    "Imagine it’s also the case that:\n",
    "- People who work more hours spend less time on the site.\n",
    "- People with more friends tend to work more hours.\n",
    "\n",
    "That is, imagine that the “actual” model is:\n",
    "\n",
    "$$ \\large \\textrm{minutes} = \\alpha + \\beta_1 \\textrm{friends} + \\beta_2 \\textrm{work hours} + \\epsilon $$\n",
    "\n",
    "where $\\beta_2$ is negative, and that work hours and friends are positively correlated. In that\n",
    "case, when we minimize the errors of the single-variable model:\n",
    "\n",
    "$$ \\large \\textrm{minutes} = \\alpha + \\beta_1 \\textrm{friends} + \\epsilon $$\n",
    "\n",
    "we will underestimate $\\beta_1$ (since $\\beta_2$, a negative value, is missing, so $\\beta_1$ alone has to be lower than what it should be to account for the loss of the negative term $\\beta_2 \\textrm{work hours}$).\n",
    "\n",
    "Think about what would happen if we made predictions using the single-variable\n",
    "model with the “actual” value of $\\beta_1$. (That is, the value that arises from minimizing\n",
    "the errors of what we called the “actual” model.)\n",
    "\n",
    "The predictions would tend to be\n",
    "way too large for users who work many hours and a little too large for users who\n",
    "work few hours, because $\\beta_2 < 0$ and we “forgot” to include it. \n",
    "\n",
    "Because work hours is\n",
    "positively correlated with number of friends, this means the predictions tend to be\n",
    "way too large for users with many friends, and only slightly too large for users with\n",
    "few friends.\n",
    "\n",
    "The result of this is that we can reduce the errors (in the single-variable model) by\n",
    "decreasing our estimate of $\\beta_1$, which means that the error-minimizing $\\beta_1$ is smaller\n",
    "than the “actual” value. \n",
    "\n",
    "That is, in this case the single-variable least squares solution is\n",
    "biased to underestimate $\\beta_1$. And, in general, whenever the independent variables are\n",
    "correlated with the errors like this, our least squares solution will give us a biased estimate\n",
    "of $\\beta_1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we did in the simple linear regression model, we’ll choose `beta` to minimize the sum of\n",
    "squared errors. Finding an exact solution is not simple to do by hand, which means\n",
    "we’ll need to use an optimizer (minimizer). Again we’ll want to minimize the sum of the\n",
    "squared errors. The error function is almost identical to the one we used in the simple linear regression model, except that instead of expecting parameters `[alpha, beta]` it will take a vector\n",
    "of arbitrary length:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "36\n"
     ]
    }
   ],
   "source": [
    "def error(x: Vector, y: float, beta: Vector) -> float:\n",
    "    return y - predict(x, beta)\n",
    "\n",
    "def squared_error(x: Vector, y: float, beta: Vector) -> float:\n",
    "    return error(x, y, beta) ** 2\n",
    "\n",
    "def sum_of_sqerrors(xs: List[Vector], ys: Vector, beta: Vector) -> float:\n",
    "    return sum(squared_error(x_i, y_i, beta) for x_i, y_i in zip(xs, ys))\n",
    "\n",
    "x = [1, 2, 3]\n",
    "y = 30\n",
    "beta = [4, 4, 4]  # so prediction = 4 + 8 + 12 = 24\n",
    "\n",
    "print(error(x, y, beta))         # 30 - 24 = 6\n",
    "print(squared_error(x, y, beta)) # 6^2 = 36"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we’re ready to find the optimal `beta` using an optimizer. Let’s first\n",
    "write out a `least_squares_fit` function that can work with any dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "def least_squares_fit(xs: List[Vector], ys: Vector) -> Vector:\n",
    "    \"\"\"\n",
    "    Find the beta that minimizes the sum of squared errors\n",
    "    assuming the model y = dot(x, beta).\n",
    "    \"\"\"\n",
    "    def object_fun (beta: Vector):\n",
    "        return sum_of_sqerrors (xs, ys, beta)\n",
    "    \n",
    "    # Start with a random guess\n",
    "    guess = [random.random() for _ in xs[0]]\n",
    "\n",
    "    optim = minimize (object_fun, guess, method='BFGS')\n",
    "    \n",
    "    return optim.x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then apply that to our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[30.57901802  0.97250517 -1.86503637  0.92320089]\n"
     ]
    }
   ],
   "source": [
    "inputs: List[Vector] = [[1.,49,4,0],[1,41,9,0],[1,40,8,0],[1,25,6,0],[1,21,1,0],[1,21,0,0],[1,19,3,0],[1,19,0,0],[1,18,9,0],[1,18,8,0],[1,16,4,0],[1,15,3,0],[1,15,0,0],[1,15,2,0],[1,15,7,0],[1,14,0,0],[1,14,1,0],[1,13,1,0],[1,13,7,0],[1,13,4,0],[1,13,2,0],[1,12,5,0],[1,12,0,0],[1,11,9,0],[1,10,9,0],[1,10,1,0],[1,10,1,0],[1,10,7,0],[1,10,9,0],[1,10,1,0],[1,10,6,0],[1,10,6,0],[1,10,8,0],[1,10,10,0],[1,10,6,0],[1,10,0,0],[1,10,5,0],[1,10,3,0],[1,10,4,0],[1,9,9,0],[1,9,9,0],[1,9,0,0],[1,9,0,0],[1,9,6,0],[1,9,10,0],[1,9,8,0],[1,9,5,0],[1,9,2,0],[1,9,9,0],[1,9,10,0],[1,9,7,0],[1,9,2,0],[1,9,0,0],[1,9,4,0],[1,9,6,0],[1,9,4,0],[1,9,7,0],[1,8,3,0],[1,8,2,0],[1,8,4,0],[1,8,9,0],[1,8,2,0],[1,8,3,0],[1,8,5,0],[1,8,8,0],[1,8,0,0],[1,8,9,0],[1,8,10,0],[1,8,5,0],[1,8,5,0],[1,7,5,0],[1,7,5,0],[1,7,0,0],[1,7,2,0],[1,7,8,0],[1,7,10,0],[1,7,5,0],[1,7,3,0],[1,7,3,0],[1,7,6,0],[1,7,7,0],[1,7,7,0],[1,7,9,0],[1,7,3,0],[1,7,8,0],[1,6,4,0],[1,6,6,0],[1,6,4,0],[1,6,9,0],[1,6,0,0],[1,6,1,0],[1,6,4,0],[1,6,1,0],[1,6,0,0],[1,6,7,0],[1,6,0,0],[1,6,8,0],[1,6,4,0],[1,6,2,1],[1,6,1,1],[1,6,3,1],[1,6,6,1],[1,6,4,1],[1,6,4,1],[1,6,1,1],[1,6,3,1],[1,6,4,1],[1,5,1,1],[1,5,9,1],[1,5,4,1],[1,5,6,1],[1,5,4,1],[1,5,4,1],[1,5,10,1],[1,5,5,1],[1,5,2,1],[1,5,4,1],[1,5,4,1],[1,5,9,1],[1,5,3,1],[1,5,10,1],[1,5,2,1],[1,5,2,1],[1,5,9,1],[1,4,8,1],[1,4,6,1],[1,4,0,1],[1,4,10,1],[1,4,5,1],[1,4,10,1],[1,4,9,1],[1,4,1,1],[1,4,4,1],[1,4,4,1],[1,4,0,1],[1,4,3,1],[1,4,1,1],[1,4,3,1],[1,4,2,1],[1,4,4,1],[1,4,4,1],[1,4,8,1],[1,4,2,1],[1,4,4,1],[1,3,2,1],[1,3,6,1],[1,3,4,1],[1,3,7,1],[1,3,4,1],[1,3,1,1],[1,3,10,1],[1,3,3,1],[1,3,4,1],[1,3,7,1],[1,3,5,1],[1,3,6,1],[1,3,1,1],[1,3,6,1],[1,3,10,1],[1,3,2,1],[1,3,4,1],[1,3,2,1],[1,3,1,1],[1,3,5,1],[1,2,4,1],[1,2,2,1],[1,2,8,1],[1,2,3,1],[1,2,1,1],[1,2,9,1],[1,2,10,1],[1,2,9,1],[1,2,4,1],[1,2,5,1],[1,2,0,1],[1,2,9,1],[1,2,9,1],[1,2,0,1],[1,2,1,1],[1,2,1,1],[1,2,4,1],[1,1,0,1],[1,1,2,1],[1,1,2,1],[1,1,5,1],[1,1,3,1],[1,1,10,1],[1,1,6,1],[1,1,0,1],[1,1,8,1],[1,1,6,1],[1,1,4,1],[1,1,9,1],[1,1,9,1],[1,1,4,1],[1,1,2,1],[1,1,9,1],[1,1,0,1],[1,1,8,1],[1,1,6,1],[1,1,1,1],[1,1,1,1],[1,1,5,1]]\n",
    "\n",
    "daily_minutes_good = [68.77,51.25,52.08,38.36,44.54,57.13,51.4,41.42,31.22,34.76,54.01,38.79,47.59,49.1,27.66,41.03,\n",
    "                     36.73,48.65,28.12,46.62,35.57,32.98,35,26.07,23.77,39.73,40.57,31.65,31.21,36.32,20.45,21.93,26.02,\n",
    "                     27.34,23.49,46.94,30.5,33.8,24.23,21.4,27.94,32.24,40.57,25.07,19.42,22.39,18.42,46.96,23.72,26.41,\n",
    "                     26.97,36.76,40.32,35.02,29.47,30.2,31,38.11,38.18,36.31,21.03,30.86,36.07,28.66,29.08,37.28,15.28,\n",
    "                     24.17,22.31,30.17,25.53,19.85,35.37,44.6,17.23,13.47,26.33,35.02,32.09,24.81,19.33,28.77,24.26,31.98,\n",
    "                     25.73,24.86,16.28,34.51,15.23,39.72,40.8,26.06,35.76,34.76,16.13,44.04,18.03,19.65,32.62,35.59,39.43,\n",
    "                     14.18,35.24,40.13,41.82,35.45,36.07,43.67,24.61,20.9,21.9,18.79,27.61,27.21,26.61,29.77,20.59,27.53,\n",
    "                     13.82,33.2,25,33.1,36.65,18.63,14.87,22.2,36.81,25.53,24.62,26.25,18.21,28.08,19.42,29.79,32.8,35.99,\n",
    "                     28.32,27.79,35.88,29.06,36.28,14.1,36.63,37.49,26.9,18.58,38.48,24.48,18.95,33.55,14.24,29.04,32.51,\n",
    "                     25.63,22.22,19,32.73,15.16,13.9,27.2,32.01,29.27,33,13.74,20.42,27.32,18.23,35.35,28.48,9.08,24.62,\n",
    "                     20.12,35.26,19.92,31.02,16.49,12.16,30.7,31.22,34.65,13.13,27.51,33.2,31.57,14.1,33.42,17.44,10.12,\n",
    "                     24.42,9.82,23.39,30.93,15.03,21.67,31.09,33.29,22.61,26.89,23.48,8.38,27.81,32.35,23.84]\n",
    "\n",
    "\n",
    "beta = least_squares_fit(inputs, daily_minutes_good)\n",
    "print(beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, you wouldn’t estimate a linear regression using an optimizer; you’d get\n",
    "the exact coefficients using linear algebra and calculus techniques that are beyond the scope of this\n",
    "class. If you did so, you’d find the equation:\n",
    "\n",
    "$$ \\textrm{minutes} = 30.58 + 0.972 \\textrm{ friends} − 1.87 \\textrm{ work hours} + 0.923 \\textrm{ phd} $$\n",
    "\n",
    "which is pretty close to what we found."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpreting the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should think of the coefficients of the model as representing all-else-being-equal\n",
    "estimates of the impacts of each factor.\n",
    "- All else being equal, each additional friend corresponds to an extra minute spent on the site each day.\n",
    "- All else being equal, each additional hour in a user’s workday corresponds to about two fewer minutes spent on the site each day.\n",
    "- All else being equal, having a PhD is associated with spending an extra minute on the site each day.\n",
    "\n",
    "What this doesn’t (directly) tell us is anything about the interactions among the variables.\n",
    "It’s possible that the effect of work hours is different for people with many\n",
    "friends than it is for people with few friends. This model doesn’t capture that. \n",
    "One way to handle this case is to introduce a new variable that is the *product* of “friends”\n",
    "and “work hours.”\n",
    "\n",
    "Or it’s possible that the more friends you have, the more time you spend on the site\n",
    "up to a point, after which further friends cause you to spend less time on the site.\n",
    "(Perhaps with too many friends the experience is just too overwhelming?) We could\n",
    "try to capture this in our model by adding another variable that’s the square of the\n",
    "number of friends.\n",
    "\n",
    "Once we start adding variables, we need to worry about whether their coefficients\n",
    "“matter.” There are no limits to the numbers of products, logs, squares, and higher\n",
    "powers we could add."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goodness of Fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again we can look at the R-squared:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x = [-100, -90, -80, -70, -60, -50, -40, -30, -20, -10, 0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
      "y = [-305, -275, -245, -215, -185, -155, -125, -95, -65, -35, -5, 25, 55, 85, 115, 145, 175, 205, 235, 265, 295]\n",
      "alpha, beta = (-5.0, 3.0)\n",
      "alpha = 22.947552413468976\n",
      "beta  = 0.9038659456058724\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rsq    = 0.3291078377836305\n",
      "rsq_sk = 0.32910783778362984\n",
      "r      = 0.57367921156656\n",
      "r^2    = 0.3291078377836299\n",
      "22.947552413468976\n",
      "0.9038659456058724\n",
      "[22.94755232  0.90386594]\n"
     ]
    }
   ],
   "source": [
    "from Week07T import total_sum_of_squares\n",
    "\n",
    "def multiple_r_squared(xs: List[Vector], ys: Vector, beta: Vector) -> float:\n",
    "    sum_of_squared_errors = sum(error(x, y, beta) ** 2\n",
    "                                for x, y in zip(xs, ys))\n",
    "    return 1.0 - sum_of_squared_errors / total_sum_of_squares(ys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which has now increased to 0.68:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6800110181375771\n"
     ]
    }
   ],
   "source": [
    "print(multiple_r_squared(inputs, daily_minutes_good, beta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep in mind, however, that adding new variables to a regression will *necessarily*\n",
    "increase the R-squared. After all, the simple regression model is just the special case\n",
    "of the multiple regression model where the coefficients on “work hours” and “PhD”\n",
    "both equal 0. The optimal multiple regression model will necessarily have an error at\n",
    "least as small as that one.\n",
    "\n",
    "Because of this, in a multiple regression, we also need to look at the *standard errors* of\n",
    "the coefficients, which measure how certain we are about our estimates of each $\\beta_i$.\n",
    "The regression as a whole may fit our data very well, but if some of the independent\n",
    "variables are correlated (or irrelevant), their coefficients might not *mean* much.\n",
    "\n",
    "The typical approach to measuring these errors starts with another assumption—that\n",
    "the errors $\\epsilon_i$ are independent normal random variables with mean 0 and some shared\n",
    "(unknown) standard deviation $\\sigma$. In that case, we (or, more likely, our statistical software)\n",
    "can use some linear algebra to find the standard error of each coefficient. The\n",
    "larger it is, the less sure our model is about that coefficient. Unfortunately, we’re not\n",
    "set up to do that kind of linear algebra from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Digression: The Bootstrap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine that we have a sample of $n$ data points, generated by some (unknown to us)\n",
    "distribution:\n",
    "\n",
    "```python\n",
    "data = get_sample(num_points=n)\n",
    "```\n",
    "\n",
    "Earlier in the semester, we wrote a function that could compute the `median` of the sample,\n",
    "which we can use as an estimate of the median of the distribution itself.\n",
    "\n",
    "But how confident can we be about our estimate? If all the data points in the sample\n",
    "are very close to 100, then it seems likely that the actual median is close to 100. If\n",
    "approximately half the data points in the sample are close to 0 and the other half are\n",
    "close to 200, then we can’t be nearly as certain about the median.\n",
    "\n",
    "If we could repeatedly get new samples, we could compute the medians of many samples\n",
    "and look at the distribution of those medians. Often we can’t. In that case we can\n",
    "*bootstrap* new datasets by choosing *n* data points *with replacement* from our data.\n",
    "And then we can compute the medians of those synthetic datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypeVar, Callable\n",
    "\n",
    "X = TypeVar('X')        # Generic type for data\n",
    "Stat = TypeVar('Stat')  # Generic type for \"statistic\"\n",
    "\n",
    "def bootstrap_sample(data: List[X]) -> List[X]:\n",
    "    \"\"\"randomly samples len(data) elements with replacement\"\"\"\n",
    "    return [random.choice(data) for _ in data]\n",
    "\n",
    "def bootstrap_statistic(data: List[X],\n",
    "                        stats_fn: Callable[[List[X]], Stat],\n",
    "                        num_samples: int) -> List[Stat]:\n",
    "    \"\"\"evaluates stats_fn on num_samples bootstrap samples from data\"\"\"\n",
    "    return [stats_fn(bootstrap_sample(data)) for _ in range(num_samples)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, consider the two following datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 101 points all very close to 100\n",
    "close_to_100 = [99.5 + random.random() for _ in range(101)]\n",
    "\n",
    "# 101 points, 50 of them near 0, 50 of them near 200\n",
    "far_from_100 = ([99.5 + random.random()] +\n",
    "                [random.random() for _ in range(50)] +\n",
    "                [200 + random.random() for _ in range(50)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you compute the medians of the two datasets, both will be very close to 100. However,\n",
    "if you look at:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100.07695233595796, 100.04096865985923, 100.0732302247859, 100.0732302247859, 99.9031154752554, 99.94477231158407, 99.82821568533592, 100.01662771062595, 99.95308613140143, 100.04096865985923, 100.07695233595796, 99.95890526554467, 99.95308613140143, 100.12360169937197, 100.04096865985923, 100.1264868989267, 99.92833829394239, 100.01662771062595, 100.04096865985923, 100.05128683971144, 100.0732302247859, 99.94092482987332, 100.06279397921298, 100.05128683971144, 99.97219856368471, 100.01662771062595, 100.00476710359703, 100.0732302247859, 100.01662771062595, 100.1264868989267, 100.06279397921298, 99.94477231158407, 99.95890526554467, 99.95890526554467, 100.00476710359703, 100.0732302247859, 100.04096865985923, 100.05128683971144, 99.95308613140143, 100.12545025949998, 99.88469254361506, 100.0732302247859, 100.05128683971144, 100.0732302247859, 99.95308613140143, 99.95890526554467, 100.0732302247859, 100.01662771062595, 99.97219856368471, 100.00476710359703, 99.92833829394239, 100.12545025949998, 100.12360169937197, 100.00476710359703, 100.00476710359703, 99.94477231158407, 100.08239195426084, 100.11291881765978, 100.01662771062595, 100.00476710359703, 100.01662771062595, 100.01662771062595, 100.04096865985923, 100.06279397921298, 100.05128683971144, 99.95890526554467, 100.04096865985923, 100.07695233595796, 100.00476710359703, 100.00476710359703, 99.92833829394239, 99.95308613140143, 100.00476710359703, 100.07695233595796, 99.97219856368471, 99.94477231158407, 100.05128683971144, 100.12360169937197, 99.94477231158407, 99.94092482987332, 100.05128683971144, 100.00476710359703, 99.9031154752554, 100.11291881765978, 100.00476710359703, 100.05128683971144, 100.07695233595796, 99.94477231158407, 100.09300154359775, 100.01662771062595, 99.95890526554467, 100.05128683971144, 99.97219856368471, 100.0732302247859, 99.97219856368471, 99.95308613140143, 99.95890526554467, 100.06279397921298, 99.95308613140143, 99.95890526554467]\n"
     ]
    }
   ],
   "source": [
    "from statistics import median, stdev\n",
    "\n",
    "medians_close = bootstrap_statistic(close_to_100, median, 100)\n",
    "print(medians_close)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "you will mostly see numbers really close to 100. But if you look at:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[200.06960447578828, 200.06927071443147, 0.9023183165976639, 200.0028814399315, 200.06927071443147, 0.9048673083118102, 200.0307730364306, 200.0307730364306, 200.23331479260912, 0.8864449752916473, 200.12713378424655, 0.9048673083118102, 0.8718848350182556, 200.06927071443147, 100.11670099633042, 200.0028814399315, 100.11670099633042, 200.11869799256763, 200.06927071443147, 200.06960447578828, 200.25791549634286, 0.9656962396104048, 200.08427261503604, 200.23331479260912, 200.23331479260912, 200.06960447578828, 0.9483290508646044, 200.0307730364306, 200.08427261503604, 0.9586187975220156, 0.9586187975220156, 0.8959158984869848, 200.06927071443147, 200.06927071443147, 0.9023183165976639, 0.9483290508646044, 0.9483290508646044, 200.0307730364306, 200.203880548764, 0.9586187975220156, 200.0028814399315, 200.0307730364306, 0.6839939579556614, 0.8864449752916473, 200.0028814399315, 0.9483290508646044, 0.8959158984869848, 200.30217167872487, 0.8214917330448126, 0.9586187975220156, 0.9586187975220156, 0.8864449752916473, 200.12713378424655, 200.11869799256763, 200.12713378424655, 200.06927071443147, 0.9048673083118102, 0.9023183165976639, 200.06927071443147, 0.9048673083118102, 200.0307730364306, 200.06960447578828, 0.9586187975220156, 0.744340962186923, 0.8718848350182556, 0.8959158984869848, 0.9483290508646044, 0.9048673083118102, 0.9586187975220156, 0.9656962396104048, 200.2335552386983, 200.25791549634286, 0.8959158984869848, 200.203880548764, 0.9048673083118102, 0.9023183165976639, 0.8718848350182556, 200.06927071443147, 0.9656962396104048, 100.11670099633042, 200.0307730364306, 0.9586187975220156, 200.06960447578828, 200.11869799256763, 100.11670099633042, 200.12713378424655, 200.0307730364306, 200.0307730364306, 200.06927071443147, 200.11869799256763, 200.0307730364306, 200.0028814399315, 200.21336813885517, 200.0028814399315, 0.9586187975220156, 0.9483290508646044, 0.9023183165976639, 200.06960447578828, 200.08427261503604, 0.8959158984869848]\n"
     ]
    }
   ],
   "source": [
    "medians_far = bootstrap_statistic(far_from_100, median, 100)\n",
    "print(medians_far)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "you will see a lot of numbers close to 0 and a lot of numbers close to 200.\n",
    "\n",
    "The standard deviation of the first set of medians is close to 0, while that of the\n",
    "second set of medians is close to 100:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.06293632405439407\n",
      "97.56117694174355\n"
     ]
    }
   ],
   "source": [
    "print(stdev(medians_close))\n",
    "print(stdev(medians_far))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(This extreme a case would be pretty easy to figure out by manually inspecting the\n",
    "data, but in general that won’t be true.)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
