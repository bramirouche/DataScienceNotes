{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees (Continued)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "1.0\n",
      "0.8112781244591328\n",
      "0.8112781244591328\n",
      "0.28639695711595625\n",
      "0.0\n",
      "1.0\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "from Week09M import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The VP provides you with the interviewee data, consisting of (per your specification)\n",
    "a `NamedTuple` of the relevant attributes for each candidate—level, preferred\n",
    "language, active on Twitter or not, has a PhD or not, and whether the candidate interviewed well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple, Optional\n",
    "\n",
    "class Candidate(NamedTuple):\n",
    "    level: str\n",
    "    lang: str\n",
    "    tweets: bool\n",
    "    phd: bool\n",
    "    did_well: Optional[bool] = None  # allow unlabeled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "                  #  level     lang     tweets  phd  did_well\n",
    "inputs = [Candidate('Senior', 'Java',   False, False, False),\n",
    "          Candidate('Senior', 'Java',   False, True,  False),\n",
    "          Candidate('Mid',    'Python', False, False, True),\n",
    "          Candidate('Junior', 'Python', False, False, True),\n",
    "          Candidate('Junior', 'R',      True,  False, True),\n",
    "          Candidate('Junior', 'R',      True,  True,  False),\n",
    "          Candidate('Mid',    'R',      True,  True,  True),\n",
    "          Candidate('Senior', 'Python', False, False, False),\n",
    "          Candidate('Senior', 'R',      True,  False, True),\n",
    "          Candidate('Junior', 'Python', True,  False, True),\n",
    "          Candidate('Senior', 'Python', True,  True,  True),\n",
    "          Candidate('Mid',    'Python', False, True,  True),\n",
    "          Candidate('Mid',    'Java',   True,  False, True),\n",
    "          Candidate('Junior', 'Python', False, True,  False)\n",
    "         ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our tree will consist of *decision nodes* (which ask a question and direct us differently\n",
    "depending on the answer) and leaf nodes (which give us a prediction).\n",
    "\n",
    "We will build it\n",
    "using the relatively simple ID3 algorithm, which operates in the following manner.\n",
    "Let’s say we’re given some labeled data, and a list of attributes to consider branching\n",
    "on:\n",
    "\n",
    "- If the data all have the same label, create a leaf node that predicts that label and then stop.\n",
    "- If the list of attributes is empty (i.e., there are no more possible questions to ask), create a leaf node that predicts the most common label and then stop.\n",
    "- Otherwise, try partitioning the data by each of the attributes.\n",
    "- Choose the partition with the lowest partition entropy.\n",
    "- Add a decision node based on the chosen attribute.\n",
    "- Recur on each partitioned subset using the remaining attributes.\n",
    "\n",
    "This is what’s known as a “greedy” algorithm because, at each step, it chooses the\n",
    "most immediately best option. Given a dataset, there may be a better tree with a\n",
    "worse-looking first move. If so, this algorithm won’t find it. Nonetheless, it is relatively\n",
    "easy to understand and implement, which makes it a good place to begin\n",
    "exploring decision trees.\n",
    "\n",
    "Let’s manually go through these steps on the interviewee dataset. The dataset has both\n",
    "`True` and `False` labels, and we have four attributes we can split on. So our first step\n",
    "will be to find the partition with the least entropy. We’ll start by writing a function\n",
    "that does the partitioning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, TypeVar\n",
    "from collections import defaultdict\n",
    "\n",
    "T = TypeVar('T')  # generic type for inputs/data point, for our purposes, T represents a candidate\n",
    "\n",
    "def partition_by(inputs: List[T], attribute: str) -> Dict[Any, List[T]]:\n",
    "    \"\"\"Partition the inputs into lists based on the specified attribute.\"\"\"\n",
    "    partitions: Dict[Any, List[T]] = defaultdict(list) # the keys are the possible values for a given attribute to split on\n",
    "    for input in inputs:\n",
    "        # getattr() returns value of the named attribute of the given object, you can think of it as Python's built-in getter \n",
    "        # for any attribute defined within a class\n",
    "        # e.g., Given the first candidate object as 'input' and attribute is 'level', getattr() would return 'Senior'\n",
    "        key = getattr(input, attribute)  # value of the specified attribute\n",
    "        partitions[key].append(input)    # add input to the correct partition\n",
    "    return partitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and one that uses it to compute entropy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition_entropy_by(inputs: List[T],\n",
    "                         attribute: str,\n",
    "                         label_attribute: str) -> float:\n",
    "    \"\"\"Compute the entropy corresponding to the given partition\"\"\"\n",
    "    # partitions consist of our inputs\n",
    "    partitions = partition_by(inputs, attribute)\n",
    "\n",
    "    # but partition_entropy needs just the class labels\n",
    "    labels = [[getattr(input, label_attribute) for input in partition]\n",
    "              for partition in partitions.values()]\n",
    "\n",
    "    return partition_entropy(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we just need to find the minimum-entropy partition for the whole dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "level 0.6935361388961919\n",
      "lang 0.8601317128547441\n",
      "tweets 0.7884504573082896\n",
      "phd 0.8921589282623617\n"
     ]
    }
   ],
   "source": [
    "for key in ['level','lang','tweets','phd']:\n",
    "    print(key, partition_entropy_by(inputs, key, 'did_well'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lowest entropy comes from splitting on `level`, so we’ll need to make a subtree\n",
    "for each possible level value.\n",
    "\n",
    "Every `Mid` candidate is labeled True, which means that\n",
    "the `Mid` subtree is simply a leaf node predicting `True`. For Senior candidates, we have\n",
    "a mix of `True`s and `False`s, so we need to split again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lang 0.4\n",
      "tweets 0.0\n",
      "phd 0.9509775004326938\n"
     ]
    }
   ],
   "source": [
    "senior_inputs = [input for input in inputs if input.level == 'Senior']\n",
    "\n",
    "for key in ['lang','tweets','phd']:\n",
    "    print(key, partition_entropy_by(senior_inputs, key, 'did_well'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows us that our next split should be on `tweets`, which results in a zeroentropy\n",
    "partition. For these `Senior`-level candidates, “yes” tweets always result in\n",
    "`True` while “no” tweets always result in `False`.\n",
    "\n",
    "We will also do the same thing for the Junior candidates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lang 0.9509775004326938\n",
      "tweets 0.9509775004326938\n",
      "phd 0.0\n"
     ]
    }
   ],
   "source": [
    "junior_inputs = [input for input in inputs if input.level == 'Junior']\n",
    "\n",
    "for key in ['lang','tweets','phd']:\n",
    "    print(key, partition_entropy_by(junior_inputs, key, 'did_well'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows that we should split on `phd`, after which we find that no PhD always results in `True` and PhD always results in\n",
    "`False`.\n",
    "\n",
    "The complete decision tree would then look like this:\n",
    "\n",
    "![The decision tree for hiring](hiring_tree.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting It All Together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we’ve seen how the algorithm works, we would like to implement it more\n",
    "generally. This means we need to decide how we want to represent trees. We’ll use\n",
    "pretty much use the most lightweight representation possible. We define a tree to be\n",
    "either:\n",
    "- a `Leaf` (that predicts a single value), or\n",
    "- a `Split` (containing an attribute to split on, subtrees for specific values of that attribute, and possibly a default value to use if we see an unknown value)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple, Union, Any\n",
    "\n",
    "class Leaf(NamedTuple):\n",
    "    value: Any\n",
    "\n",
    "class Split(NamedTuple):\n",
    "    attribute: str\n",
    "    subtrees: dict\n",
    "    default_value: Any = None\n",
    "\n",
    "DecisionTree = Union[Leaf, Split]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this representation, our hiring tree would look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "hiring_tree = Split('level', {   # First, consider \"level\".\n",
    "    'Junior': Split('phd', {     # if level is \"Junior\", next look at \"phd\"\n",
    "        False: Leaf(True),       #   if \"phd\" is False, predict True\n",
    "        True: Leaf(False)        #   if \"phd\" is True, predict False\n",
    "    }),\n",
    "    'Mid': Leaf(True),           # if level is \"Mid\", just predict True\n",
    "    'Senior': Split('tweets', {  # if level is \"Senior\", look at \"tweets\"\n",
    "        False: Leaf(False),      #   if \"tweets\" is False, predict False\n",
    "        True: Leaf(True)         #   if \"tweets\" is True, predict True\n",
    "    })\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There’s still the question of what to do if we encounter an unexpected (or missing)\n",
    "attribute value. What should our hiring tree do if it encounters a candidate whose\n",
    "`level` is `Intern`? We’ll handle this case by populating the `default_value` attribute\n",
    "with the most common label.\n",
    "\n",
    "Given such a representation, we can classify an input with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(tree: DecisionTree, input: Any) -> Any:\n",
    "    \"\"\"classify the input using the given decision tree\"\"\"\n",
    "\n",
    "    # If this is a leaf node, return its value\n",
    "    if isinstance(tree, Leaf):\n",
    "        return tree.value\n",
    "\n",
    "    # Otherwise this tree consists of an attribute to split on\n",
    "    # and a dictionary whose keys are values of that attribute\n",
    "    # and whose values of are subtrees to consider next\n",
    "    subtree_key = getattr(input, tree.attribute)\n",
    "\n",
    "    if subtree_key not in tree.subtrees:   # If no subtree for key,\n",
    "        return tree.default_value          # return the default value.\n",
    "\n",
    "    subtree = tree.subtrees[subtree_key]   # Choose the appropriate subtree\n",
    "    return classify(subtree, input)        # and use it to classify the input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All that’s left is to build the tree representation from our training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = False # used for printing out optional outputs below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tree_id3(inputs: List[Any],\n",
    "                   split_attributes: List[str],\n",
    "                   target_attribute: str) -> DecisionTree:\n",
    "    # Count target labels\n",
    "    label_counts = Counter(getattr(input, target_attribute)\n",
    "                           for input in inputs)\n",
    "    if (DEBUG): print(f\"label_counts = {label_counts}\")\n",
    "    most_common_label = label_counts.most_common(1)[0][0]\n",
    "    if (DEBUG): print(f\"label_counts.most_common(1) = {label_counts.most_common(1)}\")\n",
    "\n",
    "    # If there's a unique label, predict it\n",
    "    if len(label_counts) == 1:\n",
    "        return Leaf(most_common_label)\n",
    "\n",
    "    # If no split attributes left, return the majority label\n",
    "    if not split_attributes:\n",
    "        return Leaf(most_common_label)\n",
    "\n",
    "    # Otherwise split by the best attribute\n",
    "\n",
    "    def split_entropy(attribute: str) -> float:\n",
    "        \"\"\"Helper function for finding the best attribute\"\"\"\n",
    "        return partition_entropy_by(inputs, attribute, target_attribute)\n",
    "\n",
    "    # for each attribute in split_attributes, get the partition's entropy\n",
    "    # then sort the all the attributes by the entropy\n",
    "    best_attribute = min(split_attributes, key=split_entropy)\n",
    "\n",
    "    # get the actual partition by the best_attribute\n",
    "    partitions = partition_by(inputs, best_attribute)\n",
    "    \n",
    "    # remove the best_attribute from the current list of split_attributes\n",
    "    new_attributes = [a for a in split_attributes if a != best_attribute]\n",
    "\n",
    "    # recursively build the subtrees\n",
    "    subtrees = {attribute_value : build_tree_id3(subset,\n",
    "                                                 new_attributes,\n",
    "                                                 target_attribute)\n",
    "                for attribute_value, subset in partitions.items()}\n",
    "\n",
    "    return Split(best_attribute, subtrees, default_value=most_common_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the tree we built, every leaf consisted entirely of True inputs or entirely of False\n",
    "inputs. This means that the tree predicts perfectly on the training dataset. But we can\n",
    "also apply it to new data that wasn’t in the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = build_tree_id3(inputs,\n",
    "                      ['level', 'lang', 'tweets', 'phd'],\n",
    "                      'did_well')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# Should predict True\n",
    "print(classify(tree, Candidate(\"Junior\", \"Java\", True, False)))\n",
    "\n",
    "# Should predict False\n",
    "print(classify(tree, Candidate(\"Junior\", \"Java\", True, True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And also to data with unexpected values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# Should predict True because \"Intern\" does not exists in the tree and the most common (default) value is True\n",
    "print(classify(tree, Candidate(\"Intern\", \"Java\", True, True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given how closely decision trees can fit themselves to their training data, it’s not surprising\n",
    "that they have a tendency to overfit. One way of avoiding this is a technique\n",
    "called *random forests*, in which we build multiple decision trees and combine their outputs.\n",
    "- if they’re classification trees, we might let them vote;\n",
    "- if they’re regression trees, we might average their predictions.\n",
    "\n",
    "Our tree-building process was deterministic, so how do we get random trees?\n",
    "\n",
    "One piece involves *bootstrapping* data (discussed earlier in Regression). Rather than training each tree on all the inputs in the training set, we train\n",
    "each tree on the result of `bootstrap_sample(inputs)`. Since each tree is built using\n",
    "different data, each tree will be different from every other tree.\n",
    "\n",
    "A second source of randomness involves changing the way we choose the\n",
    "`best_attribute` to split on. Rather than looking at all the remaining attributes, we\n",
    "first choose a random subset of them and then split on whichever of those is best:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# if there are already few enough split candidates, look at all of them\n",
    "if len(split_candidates) <= self.num_split_candidates:\n",
    "    sampled_split_candidates = split_candidates\n",
    "# otherwise pick a random sample\n",
    "else:\n",
    "    sampled_split_candidates = random.sample(split_candidates,\n",
    "                                             self.num_split_candidates)\n",
    "    \n",
    "# now choose the best attribute only from those candidates\n",
    "best_attribute = min(sampled_split_candidates, key=split_entropy)\n",
    "partitions = partition_by(inputs, best_attribute)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an example of a broader technique called *ensemble learning* in which we combine\n",
    "several weak learners (typically high-bias, low-variance models) in order to produce\n",
    "an overall strong model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
