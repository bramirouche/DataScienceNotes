{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Really Dumb Spam Filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine a “universe” that consists of receiving a message chosen randomly from all\n",
    "possible messages. Let $S$ be the event “the message is spam” and $B$ be the event “the\n",
    "message contains the word *bitcoin*.” Bayes’s theorem tells us that the probability that\n",
    "the message is spam conditional on containing the word *bitcoin* is:\n",
    "\n",
    "$$ P(S|B) = \\frac{P(B|S)P(S)}{P(B|S)P(S) + P(B|\\neg S)P(\\neg S)} $$\n",
    "\n",
    "The numerator is the probability that a message is spam *and* contains *bitcoin*, while\n",
    "the denominator is just the probability that a message contains *bitcoin*.\n",
    "\n",
    "Hence, you\n",
    "can think of this calculation as simply representing the proportion of *bitcoin* messages\n",
    "that are spam.\n",
    "\n",
    "If we have a large collection of messages we know are spam, and a large collection of\n",
    "messages we know are not spam, then we can easily estimate $P(B|S)$ and $P(B|\\neg S)$.\n",
    "\n",
    "If we further assume that any message is equally likely to be spam or not spam (so that\n",
    "$P(S) = P(\\neg S) = 0.5$), then:\n",
    "\n",
    "$$ P(S|B) = \\frac{P(B|S)}{P(B|S) + P(B|\\neg S)} $$\n",
    "\n",
    "For example, if 50% of spam messages have the word bitcoin, but only 1% of nonspam\n",
    "messages do, then the probability that any given bitcoin-containing email is spam is:\n",
    "\n",
    "$$ \\frac{0.5}{0.5 + 0.01} \\approx 0.98 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A More Sophisticated Spam Filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine now that we have a vocabulary of many words, $w_1, \\dots, w_n$. To move this into\n",
    "the realm of probability theory, we’ll write $X_i$ for the event “a message contains the\n",
    "word $w_i$.”\n",
    "\n",
    "Also imagine that (through some unspecified-at-this-point process) we’ve\n",
    "come up with an estimate $P(X_i|S)$ for the probability that a spam message contains the\n",
    "$i$th word, and a similar estimate $P(X_i|\\neg S)$ for the probability that a nonspam message\n",
    "contains the $i$th word.\n",
    "\n",
    "The key to Naive Bayes is making the (big) assumption that the presences (or absences)\n",
    "of each word are independent of one another, conditional on a message being\n",
    "spam or not.\n",
    "\n",
    "Intuitively, this assumption means that knowing whether a certain spam\n",
    "message contains the word *bitcoin* gives you no information about whether that same\n",
    "message contains the word *rolex*.\n",
    "\n",
    "In math terms, this means that:\n",
    "\n",
    "$$ P(X_1 = x_1, \\dots, X_n = x_n | S) = P(X_1 = x_1 | S) \\times \\dots \\times P(X_n = x_n | S) $$\n",
    "\n",
    "This is an extreme assumption. (There’s a reason the technique has *naive* in its name.)\n",
    "\n",
    "Imagine that our vocabulary consists only of the words *bitcoin* and *rolex*, and that half\n",
    "of all spam messages are for “earn bitcoin” and that the other half are for “authentic\n",
    "rolex.” In this case, the Naive Bayes estimate that a spam message contains both *bitcoin*\n",
    "and *rolex* is:\n",
    "\n",
    "$$ P(X_1 = 1, X_2 = 1 | S) = P(X_1 = 1 | S) P(X_2 = 1 | S) = 0.5 \\times 0.5 = 0.25 $$\n",
    "\n",
    "while in reality that bitcoin and rolex actually never occur together (in our example).\n",
    "\n",
    "Despite the unrealisticness of this assumption, this model often performs\n",
    "well and has historically been used in actual spam filters.\n",
    "\n",
    "The same Bayes’s theorem reasoning we used for our “bitcoin-only” spam filter tells\n",
    "us that we can calculate the probability a message is spam using the equation:\n",
    "\n",
    "$$ P(S|X=x) = \\frac{P(X=x|S)}{P(X=x|S) + P(X=x|\\neg S)} $$\n",
    "\n",
    "The Naive Bayes assumption allows us to compute each of the probabilities on the\n",
    "right simply by multiplying together the individual probability estimates for each\n",
    "vocabulary word.\n",
    "\n",
    "In practice, you usually want to avoid multiplying lots of probabilities together, to\n",
    "prevent a problem called *underflow*, in which computers don’t deal well with floatingpoint\n",
    "numbers that are too close to 0.\n",
    "\n",
    "Recalling from algebra that $\\log(ab) = \\log(a) + \\log(b)$ and that $e^{\\log(x)} = x$, we usually compute $p_1 * \\dots * p_n$ as the equivalent (but floating-point-friendlier):\n",
    "\n",
    "$$ \\Large e^{\\log(p_1) + \\dots + \\log(p_n)} $$\n",
    "\n",
    "The only challenge left is coming up with estimates for $P(X_i|S)$ and $P(X_i|\\neg S)$, the probabilities that a spam message (or nonspam message) contains the word $w_i$. If we\n",
    "have a fair number of “training” messages labeled as spam and not spam, an obvious\n",
    "first try is to estimate $P(X_i|S)$ simply as the fraction of spam messages containing the\n",
    "word $w_i$.\n",
    "\n",
    "This causes a big problem, though. Imagine that in our training set the vocabulary\n",
    "word *data* only occurs in nonspam messages. Then we’d estimate $P(data|S) = 0$. The\n",
    "result is that our Naive Bayes classifier would always assign spam probability 0 to any\n",
    "message containing the word *data*, even a message like “data on free bitcoin and\n",
    "authentic rolex watches.” To avoid this problem, we usually use some kind of smoothing.\n",
    "\n",
    "In particular, we’ll choose a *pseudocount*—$k$—and estimate the probability of seeing\n",
    "the *i*th word in a spam message as:\n",
    "\n",
    "$$ P(X_i|S) = \\frac{\\textrm{number of spams containing } w_i + k}{\\textrm{number of spams } + 2k} $$\n",
    "\n",
    "Usually $k$ is just some very small positive number, and we've essentially added $k$ to the numerator and $2k$ to the denominator to prevent the fraction to ever become 0.\n",
    "\n",
    "We do similarly for $P(X_i | \\neg S)$. That is, when computing the spam probabilities for the\n",
    "*i*th word, we assume we also saw $k$ additional nonspams containing the word and $k$\n",
    "additional nonspams not containing the word.\n",
    "\n",
    "For example, if data occurs in 0/98 spam messages, and if $k$ is 1, we estimate $P(data|S)$\n",
    "as 1/100 = 0.01, which allows our classifier to still assign some nonzero spam probability\n",
    "to messages that contain the word data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have all the pieces we need to build our classifier. First, let’s create a simple\n",
    "function to tokenize messages into distinct words. We’ll first convert each message to\n",
    "lowercase, then use `re.findall` to extract “words” consisting of letters, numbers, and\n",
    "apostrophes. Finally, we’ll use `set` to get just the distinct words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'is', 'science', 'data'}\n"
     ]
    }
   ],
   "source": [
    "from typing import Set\n",
    "import re\n",
    "\n",
    "def tokenize(text: str) -> Set[str]:\n",
    "    text = text.lower()                         # Convert to lowercase,\n",
    "    all_words = re.findall(\"[a-z0-9']+\", text)  # extract the words, and\n",
    "    return set(all_words)                       # remove duplicates.\n",
    "\n",
    "print(tokenize(\"Data Science is science\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’ll also define a type for our training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple\n",
    "\n",
    "class Message(NamedTuple):\n",
    "    text: str\n",
    "    is_spam: bool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As our classifier needs to keep track of tokens, counts, and labels from the training\n",
    "data, we’ll make it a class. Following convention, we refer to nonspam emails as ham\n",
    "emails.\n",
    "\n",
    "Let's take a look at the `NaiveBayesClassifier` class and we will cover each part one at a time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple, Dict, Iterable\n",
    "import math\n",
    "from collections import defaultdict\n",
    "\n",
    "class NaiveBayesClassifier:\n",
    "    def __init__(self, k: float = 0.5) -> None:\n",
    "        self.k = k  # smoothing factor\n",
    "\n",
    "        self.tokens: Set[str] = set()\n",
    "        self.token_spam_counts: Dict[str, int] = defaultdict(int)\n",
    "        self.token_ham_counts: Dict[str, int] = defaultdict(int)\n",
    "        self.spam_messages = self.ham_messages = 0\n",
    "\n",
    "    def train(self, messages: Iterable[Message]) -> None:\n",
    "        for message in messages:\n",
    "            # Increment message counts\n",
    "            if message.is_spam:\n",
    "                self.spam_messages += 1\n",
    "            else:\n",
    "                self.ham_messages += 1\n",
    "\n",
    "            # Increment word counts\n",
    "            for token in tokenize(message.text):\n",
    "                self.tokens.add(token)\n",
    "                if message.is_spam:\n",
    "                    self.token_spam_counts[token] += 1\n",
    "                else:\n",
    "                    self.token_ham_counts[token] += 1\n",
    "\n",
    "    def _probabilities(self, token: str) -> Tuple[float, float]:\n",
    "        \"\"\"returns P(token | spam) and P(token | not spam)\"\"\"\n",
    "        spam = self.token_spam_counts[token] # number of spams containing the token word\n",
    "        ham = self.token_ham_counts[token]   # number of hams containing the token word\n",
    "\n",
    "        p_token_spam = (spam + self.k) / (self.spam_messages + 2 * self.k)\n",
    "        p_token_ham = (ham + self.k) / (self.ham_messages + 2 * self.k)\n",
    "\n",
    "        return p_token_spam, p_token_ham\n",
    "\n",
    "    def predict(self, text: str) -> float:\n",
    "        text_tokens = tokenize(text)\n",
    "        log_prob_if_spam = log_prob_if_ham = 0.0\n",
    "\n",
    "        # Iterate through each word in our vocabulary.\n",
    "        for token in self.tokens:\n",
    "            prob_if_spam, prob_if_ham = self._probabilities(token)\n",
    "\n",
    "            # If *token* appears in the message,\n",
    "            # add the log probability of seeing it;\n",
    "            if token in text_tokens:\n",
    "                log_prob_if_spam += math.log(prob_if_spam)\n",
    "                log_prob_if_ham += math.log(prob_if_ham)\n",
    "\n",
    "            # otherwise add the log probability of _not_ seeing it\n",
    "            # which is log(1 - probability of seeing it)\n",
    "            else:\n",
    "                log_prob_if_spam += math.log(1.0 - prob_if_spam)\n",
    "                log_prob_if_ham += math.log(1.0 - prob_if_ham)\n",
    "\n",
    "        prob_if_spam = math.exp(log_prob_if_spam)\n",
    "        prob_if_ham = math.exp(log_prob_if_ham)\n",
    "        return prob_if_spam / (prob_if_spam + prob_if_ham)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The constructor will take just one parameter, the pseudocount to use when computing\n",
    "probabilities. It also initializes an empty set of tokens, counters to track how often\n",
    "each token is seen in spam messages and ham messages, and counts of how many\n",
    "spam and ham messages it was trained on:\n",
    "```python\n",
    "    def __init__(self, k: float = 0.5) -> None:\n",
    "        self.k = k  # smoothing factor\n",
    "\n",
    "        self.tokens: Set[str] = set()\n",
    "        self.token_spam_counts: Dict[str, int] = defaultdict(int)\n",
    "        self.token_ham_counts: Dict[str, int] = defaultdict(int)\n",
    "        self.spam_messages = self.ham_messages = 0\n",
    "```\n",
    "\n",
    "Next, we’ll give it a method to train it on a bunch of messages. First, we increment the\n",
    "`spam_messages` and `ham_messages` counts. Then we tokenize each message text, and\n",
    "for each token we increment the `token_spam_counts` or `token_ham_counts` based on\n",
    "the message type:\n",
    "\n",
    "```python\n",
    "    def train(self, messages: Iterable[Message]) -> None:\n",
    "        for message in messages:\n",
    "            # Increment message counts\n",
    "            if message.is_spam:\n",
    "                self.spam_messages += 1\n",
    "            else:\n",
    "                self.ham_messages += 1\n",
    "\n",
    "            # Increment word counts\n",
    "            for token in tokenize(message.text):\n",
    "                self.tokens.add(token)\n",
    "                if message.is_spam:\n",
    "                    self.token_spam_counts[token] += 1\n",
    "                else:\n",
    "                    self.token_ham_counts[token] += 1\n",
    "```\n",
    "\n",
    "Ultimately we’ll want to predict $P(\\textrm{spam }|\\textrm{ token})$. As we saw earlier, to apply Bayes’s\n",
    "theorem we need to know $P(\\textrm{token }|\\textrm{ spam})$ and $P(\\textrm{token }|\\textrm{ ham})$ for each token in the\n",
    "vocabulary. So we’ll create a “private” helper function to compute those:\n",
    "\n",
    "```python\n",
    "    def _probabilities(self, token: str) -> Tuple[float, float]:\n",
    "        \"\"\"returns P(token | spam) and P(token | not spam)\"\"\"\n",
    "        spam = self.token_spam_counts[token] # number of spams containing the token word\n",
    "        ham = self.token_ham_counts[token]   # number of hams containing the token word\n",
    "\n",
    "        p_token_spam = (spam + self.k) / (self.spam_messages + 2 * self.k)\n",
    "        p_token_ham = (ham + self.k) / (self.ham_messages + 2 * self.k)\n",
    "\n",
    "        return p_token_spam, p_token_ham\n",
    "```\n",
    "\n",
    "Finally, we’re ready to write our `predict` method. As mentioned earlier, rather than\n",
    "multiplying together lots of small probabilities, we’ll instead sum up the log probabilities:\n",
    "\n",
    "```python\n",
    "    def predict(self, text: str) -> float:\n",
    "        text_tokens = tokenize(text)\n",
    "        log_prob_if_spam = log_prob_if_ham = 0.0\n",
    "\n",
    "        # Iterate through each word in our vocabulary.\n",
    "        for token in self.tokens:\n",
    "            prob_if_spam, prob_if_ham = self._probabilities(token)\n",
    "\n",
    "            # If *token* appears in the message,\n",
    "            # add the log probability of seeing it;\n",
    "            if token in text_tokens:\n",
    "                log_prob_if_spam += math.log(prob_if_spam)\n",
    "                log_prob_if_ham += math.log(prob_if_ham)\n",
    "\n",
    "            # otherwise add the log probability of _not_ seeing it\n",
    "            # which is log(1 - probability of seeing it)\n",
    "            else:\n",
    "                log_prob_if_spam += math.log(1.0 - prob_if_spam)\n",
    "                log_prob_if_ham += math.log(1.0 - prob_if_ham)\n",
    "\n",
    "        prob_if_spam = math.exp(log_prob_if_spam)\n",
    "        prob_if_ham = math.exp(log_prob_if_ham)\n",
    "        return prob_if_spam / (prob_if_spam + prob_if_ham)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Our Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s make sure our model works by writing some unit tests for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [Message(\"spam rules\", is_spam=True),\n",
    "            Message(\"ham rules\", is_spam=False),\n",
    "            Message(\"hello ham\", is_spam=False)]\n",
    "\n",
    "model = NaiveBayesClassifier(k=0.5)\n",
    "model.train(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let’s check that it got the counts right:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hello', 'spam', 'ham', 'rules'}\n",
      "1\n",
      "2\n",
      "defaultdict(<class 'int'>, {'spam': 1, 'rules': 1})\n",
      "defaultdict(<class 'int'>, {'ham': 2, 'rules': 1, 'hello': 1})\n"
     ]
    }
   ],
   "source": [
    "print(model.tokens)            # {\"spam\", \"ham\", \"rules\", \"hello\"}\n",
    "print(model.spam_messages)     # 1\n",
    "print(model.ham_messages)      # 2\n",
    "print(model.token_spam_counts) # {\"spam\": 1, \"rules\": 1}\n",
    "print(model.token_ham_counts)  # {\"ham\": 2, \"rules\": 1, \"hello\": 1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let’s make a prediction. We’ll also (laboriously) go through our Naive Bayes logic\n",
    "by hand, and make sure that we get the same result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8350515463917525\n",
      "0.8350515463917525\n"
     ]
    }
   ],
   "source": [
    "text = \"hello spam\"\n",
    "\n",
    "# P(token | spam) = (num of spams in training data containing token + k) / (total number of spams in training data + 2k)\n",
    "probs_if_spam = [\n",
    "        (1 + 0.5) / (1 + 2 * 0.5),  # \"spam\"  (present)\n",
    "    1 - (0 + 0.5) / (1 + 2 * 0.5),  # \"ham\"   (not present)\n",
    "    1 - (1 + 0.5) / (1 + 2 * 0.5),  # \"rules\" (not present)\n",
    "        (0 + 0.5) / (1 + 2 * 0.5)   # \"hello\" (present)\n",
    "]\n",
    "\n",
    "# P(token | ham) = (num of hams in training data containing token + k) / (total number of hams in training data + 2k)\n",
    "probs_if_ham = [\n",
    "        (0 + 0.5) / (2 + 2 * 0.5),  # \"spam\"  (present)\n",
    "    1 - (2 + 0.5) / (2 + 2 * 0.5),  # \"ham\"   (not present)\n",
    "    1 - (1 + 0.5) / (2 + 2 * 0.5),  # \"rules\" (not present)\n",
    "        (1 + 0.5) / (2 + 2 * 0.5),  # \"hello\" (present)\n",
    "]\n",
    "\n",
    "p_if_spam = math.exp(sum(math.log(p) for p in probs_if_spam))\n",
    "p_if_ham = math.exp(sum(math.log(p) for p in probs_if_ham))\n",
    "\n",
    "# Both should be about 0.83\n",
    "print(model.predict(text))\n",
    "print(p_if_spam / (p_if_spam + p_if_ham))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This test passes, so it seems like our model is doing what we think it is. If you look at\n",
    "the actual probabilities, the two big drivers are that our message contains *spam* (which our lone training spam message did) and that it doesn’t contain ham (which\n",
    "both our training ham messages did).\n",
    "\n",
    "Next we will try it on some real data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
